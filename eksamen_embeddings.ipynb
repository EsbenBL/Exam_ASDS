{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding is the vectorization task of representing word meaning through n-dimensional vectors of numerical values based on the words that frequently appear in the immediate semantic context of the word (Grimmer et al. 2022: 79).  Modern embeddings rely on the neural network architecture for this vectorization (Grimmer et al. 2022: 82). In this section, we will train a word2vec embedding model on the United Nations General Debate Corpus, and explore the strengths and weaknesses of word embedding for semantic analysis. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your own word2vec vectors on the dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load, tokenize, and lowercase the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(filename):\n",
    "    # to hold all sentences in the corpus\n",
    "    corpus=[]\n",
    "    \n",
    "    # Open the file \n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f: # iterate over lines\n",
    "            # is line empty\n",
    "            if line.strip() != '':\n",
    "                # Tokenize and lowercase \n",
    "                encoded_text=[word.lower() for word in word_tokenize(line)]\n",
    "                # Add tokens to the corpus\n",
    "                corpus.append(encoded_text)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "# load and process the datab\n",
    "path=\"Data\"\n",
    "speeches=load_and_process_data(path+\"/allspeeches_77_2022.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use word embeddings four choices is essential, data source, context window size, dimension in the embedding and algorithm/architecture. As we have already been told the specifics of data (UN speeches), window size(5), feature lengths(200) and algorithm(word2vec) we will not go more in depth with these specifications. Word2ved is an algorithm to construct document embeddings from one's data, and was introduced by Le and Milov (2014) (Grimmer et al. 2022: 86; Hovy 2022b: 19). The primary thought behind a word2vec is that words which appear in the same context also have similar meaning. Thus a word2vec constructs vectors based on a text corpus in a multidimensional space, where words which are more or less close in the multidimensional space share more or less meaning. \n",
    "\n",
    "There are two architectures which can be implemented in the Word2Vec, either Skip-Gram(SG) or Continous Bag of Words (CBOW). In general SG outperforms CBOW, so we will use that architecture for this exercise (Ogundepo 2021). Another important step of Word2Vec is the choice of approach to update the weights in the neural network within Word2Vec, and here we choose the default of negative sampling. Without any optimization, the neural network needs to update the weights for all the words in the vocabulary during each training step, which can be computationally intensive for large vocabularies. Negative sampling's solution to this is to update the weights for a small, randomly chosen subset of 'negative' words (those not present in the context) along with the 'positive' word (the actual context word). The basic idea is that, instead of updating all the words, we update only a small number of negative samples and the positive word, saving the computing power.   \n",
    "\n",
    "Besides the specifications already given us in this exercise we also set seed, select the skip-gram model, and choose to ignore low frequency words. \n",
    "\n",
    "https://medium.com/analytics-vidhya/understanding-word2vec-39fabe660705 Ogundepo, Odunayo  (2021): \"Understanding Word2Vec\",\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_function(seed):\n",
    "    speech2vec=gensim.models.Word2Vec(\n",
    "        speeches,         # the corpus object we've loaded\n",
    "        vector_size=200,  # the dimensionality of the target vectors\n",
    "        window=5,         # window ngram size\n",
    "        min_count=4,      # ignoring low-frequency words\n",
    "        epochs=3,         # how many training passes to have\n",
    "        sg=1,           # 1 for skip-gram model, 0 for cbow\n",
    "        seed=seed)        # Set seed for 2vec, to be able to replicate the data¨\n",
    "    return(speech2vec)\n",
    "vec_seed13=vec_function(seed=13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the ten words that are most similar to: “climate”, “pandemic”, “terrorism” and “future” (or choose your own four words of interest). Briefly discuss anything you find noteworthy about the associations that the model has picked up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>elizabeth</th>\n",
       "      <th>russia</th>\n",
       "      <th>ukraine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equality</td>\n",
       "      <td>king</td>\n",
       "      <td>ukraine</td>\n",
       "      <td>russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>empowerment</td>\n",
       "      <td>majesty</td>\n",
       "      <td>russian</td>\n",
       "      <td>russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>participation</td>\n",
       "      <td>queen</td>\n",
       "      <td>aggression</td>\n",
       "      <td>military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technical</td>\n",
       "      <td>abdullah</td>\n",
       "      <td>military</td>\n",
       "      <td>aggression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>protection</td>\n",
       "      <td>congratulating</td>\n",
       "      <td>illegal</td>\n",
       "      <td>forces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>public</td>\n",
       "      <td>charles</td>\n",
       "      <td>invasion</td>\n",
       "      <td>armed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>promotion</td>\n",
       "      <td>abdulla</td>\n",
       "      <td>forces</td>\n",
       "      <td>conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>women</td>\n",
       "      <td>excellency</td>\n",
       "      <td>unprovoked</td>\n",
       "      <td>war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>capital</td>\n",
       "      <td>csaba</td>\n",
       "      <td>federation</td>\n",
       "      <td>illegal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>youth</td>\n",
       "      <td>maldives</td>\n",
       "      <td>occupation</td>\n",
       "      <td>crimes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gender       elizabeth      russia     ukraine\n",
       "0       equality            king     ukraine      russia\n",
       "1    empowerment         majesty     russian     russian\n",
       "2  participation           queen  aggression    military\n",
       "3      technical        abdullah    military  aggression\n",
       "4     protection  congratulating     illegal      forces\n",
       "5         public         charles    invasion       armed\n",
       "6      promotion         abdulla      forces    conflict\n",
       "7          women      excellency  unprovoked         war\n",
       "8        capital           csaba  federation     illegal\n",
       "9          youth        maldives  occupation      crimes"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking for the ten words which are closest to climate, pandemic, terrorism, future\n",
    "word_of_interest=[\"gender\", \"elizabeth\", \"russia\", \"ukraine\"] \n",
    "\n",
    "# Constructing a function for finding the top ten most similar words\n",
    "def find_top_words(word_of_interest, model):\n",
    "    df=pd.DataFrame() \n",
    "    for word in word_of_interest:\n",
    "        similar_words=list(map(lambda x: x[0], model.wv.most_similar(word, 10))) # Using the most_similar function to find the 10 words\n",
    "        df[word]=similar_words\n",
    "    return(df) # Returning a dataframe\n",
    "\n",
    "top_words_seed13=find_top_words(word_of_interest=word_of_interest, model= vec_seed13)\n",
    "top_words_seed13"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the word2vec creates a geometric space for words, we are able to interpret the words based on their spatial relationship in the word2vec model. An essential weakness within the word2vec models is homographs such as *bat*, *tear*, or *lead* which all have two distinct different meanings, which the model would not be able to distinguish between the words. This results in a loss of specificity. In newer models such as BERT, they have implemented contextual embeddings, which are better at capturing these nuances in the language.   \n",
    "\n",
    "In the above function we have chosen the four words based on either topics of interest or based on key events 2022, `gender`, `elizabeth`, `ukraine`, and `russia`:\n",
    "\n",
    "Starting with `gender`, we see that *empowerment* and *protection* are the two most similar or closest words to gender. On a more broad note is it interesting that men is not a similar word to gender but `women` is. A Beauvoirian notion regarding this would be that men are considered the 'default' gender, and women is the 'other', the gender which is talked about when discussing gender. Another take could also be that the speeches in the UN regarding gender to a higher degree discuss the necessity of empowerment and protection of women because they need it more than men. \n",
    "The second word we look up is `elisabeth`, this is specifically with regards to queen Elisabeths death in 2022. It makes sense that *king*, *charles*, and *iii* all are close to this in the semantic space since they probably refer to her son King Charles III. The words *abdulla*/*abdullah* and *shahid* probably refers to Abdullah Shahid, a key politician within the UN. The last two words which we included are `russia` and `ukraine` because 2022 was when Russia attacked Ukraine. Both countries have each other as the most similar, and in general have multiple words in common. Furthermore are all of their words in some way or another related to the war. It is interesting that Ukraine has the word *sanction* which probably also relates to their call for sanctions towards Russia, and *armed* which could be related to their call for support of arms for defending themselves. \n",
    "\n",
    "On a final note, the above represent our considerations regarding the analysis of the nearest neighbors of the four chosen words and their corresponding 10 words. When presenting these it is important to keep in mind that the Word2Vec can't tell us anything regarding the word interpretability outside the words spatial relationship. This is a humane process which needs to be supported by logical arguments. (NOK BARE UD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppose that we would like to use these word embeddings as input in a supervised model, detecting whether the speech comes from a country in the global North or the global South. Briefly discuss the upside(s) and downside(s) of training your word embeddings locally, on the speeches themselves, versus using pre-trained embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deciding whether to use and train our own word embeddings locally or to use pre-trained embeddings an important consideration is how niché our own text corpora is compared with more general corpora which pre-trained models normally are trained on, such as Wikipedia or english dictionaries (Rodriguez & Spirling 2022: 6). The danger using a pre-trained model would be that the more niche words of the UN 2022 speeches would be placed in spatial relation with words in the more general corpora which the pre-trained model uses. The UN speech’s words could then lose their more distinct specific meaning within the UN. Another danger would be that the words in our speech corpora are not within the pre-trained model's vocabulary, and therefore would need to be handled as an out-of-vocabulary word, which represents a problem (Grimmer et al. 2022: 84f). On the other side does a locally trained model also present some problems, and the most essential consideration for us is that we are lacking in data to make a good local embedding model which is able to compete with a pre-trained embedding model. As we only have 77 speeches in the UN in 2022 for our word2vec to be trained on.  \n",
    "\n",
    "In general Rodriguez & Spirling find that the pre-trained embeddings do well even compared with human constructed nearest neighbor, and the pre-trained embeddings often do even better than the locally trained models (Rodriguez & Spirling 2022: 15-16). This would probably also be the case for this model. If one would seek the flexibility (because you to a higher degree can adjust your own parameters) and the context-specificity of a local model, a solution could be to include speeches before 2022, or maybe include english speeches from other multi country organizations (European Union, African Union), but both of these solutions would also contaminate the text corpus outside of the UN 2022 sphere."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Train the model again (set a different random seed if you used a seed in step 1), print the most similar words again, and compare your two results. Are the word embeddings stable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_seed12=vec_function(seed=12)\n",
    "top_words_seed12=find_top_words(word_of_interest= word_of_interest, model=vec_seed12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender Seed 12:\n",
      "--------------------\n",
      "0: empowerment\n",
      "1: girls\n",
      "2: protection\n",
      "3: women\n",
      "4: youth\n",
      "5: technical\n",
      "6: employment\n",
      "7: equality\n",
      "8: assistance\n",
      "9: capital\n",
      "\n",
      "Gender Seed 13:\n",
      "--------------------\n",
      "0: equality\n",
      "1: empowerment\n",
      "2: participation\n",
      "3: technical\n",
      "4: protection\n",
      "5: public\n",
      "6: promotion\n",
      "7: women\n",
      "8: capital\n",
      "9: youth\n"
     ]
    }
   ],
   "source": [
    "seed12_data=top_words_seed12[\"gender\"]\n",
    "seed13_data=top_words_seed13[\"gender\"]\n",
    "\n",
    "print(f\"Gender Seed 12:\\n{'-'*20}\")\n",
    "for key, value in seed12_data.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\nGender Seed 13:\\n{'-'*20}\")\n",
    "for key, value in seed13_data.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we run a new Word2Vec on a new seed, and print and compare for example the gender variable within the two lists generated from using either seed 12 or 13 we observe in general that the model is exactly stable. To some degree the Word2vec will often have room for some instability, as some randomizing elements are active in the model as for example the aforementioned negative sampling. But it seems the model is quite unstable and this is probably because of the small size of our text corpus. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Conduct an informal validation of the embeddings from your first run, by checking their ability to find the “odd one out” in three different series of four–five terms related to international relations and current events (e.g. “covid”, “pandemic”, “disease”, “vaccine”, “environment”). Briefly discuss how you might validate the embeddings more systematically, if you had more time and resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'war' the odd one out is: bombs\n",
      "For 'climate' the odd one out is: drought\n",
      "For 'nations' the odd one out is: america\n",
      "For 'covid 19' the odd one out is: environment\n"
     ]
    }
   ],
   "source": [
    "list_of_lists_of_words={\n",
    "    \"war\": [\"bombs\", \"guns\", \"ammo\", \"tanks\" \"cake\"],\n",
    "    \"climate\": [\"floods\", \"drought\", \"heatwave\", \"hallo\"],\n",
    "    \"nations\": [\"china\", \"america\", \"india\", \"hammer\"],\n",
    "    \"covid 19\": [\"covid\", \"pandemic\", \"disease\", \"vaccine\", \"environment\"]\n",
    "}\n",
    "for key, value in list_of_lists_of_words.items():\n",
    "    print(f\"For '{key}' the odd one out is: {vec_seed13.wv.doesnt_match(value)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the how well the model recognizes the \"odd one out\" our model is failing quite a bit. It only succeeds in finding the ´environment´ in the covid 19 list, and find the wrong one in all the other lists.  \n",
    "\n",
    "When considering the validation of embeddings models one can think of extrinsic and intrinsic evaluation for checking the validation of the embeddings. Extrinsic evaluation  is how well the embedding would do if given to a model and for this one can include, noun phrase chunking, part of speech tagging, and named entity recognition. For intrinsic evaluation the focus is on how successful the model is presenting word similarity (Grimmer et al. 2022: 87). For validating this we can look into analogy solvings and clustering tasks like the one asked from the exercise above to see how well the model succeeds in comparing different words. \n",
    "\n",
    "If one had the time and ressources the method presented by Rodriguez & Spirling (2021), who construct a nearest neighbor list for ten words using labellers and another nearest neighbor list using an embedding model (Rodriguez & Spirling 2021: 11). They then test the embedding model by other people to choose between which of the two nearest neighbor lists they think have the best nearest neighbor results, thus they can compare their embedding model to that of normal humans. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SKRALD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the DataLoader for preparing data for the word2vec\n",
    "\n",
    "class MyDataLoader(object):\n",
    "\n",
    "    # when we initialize this dataloader, it will take a filename as an argument\n",
    "    def __init__(self, filename):\n",
    "        self.corpus=filename\n",
    "\n",
    "    # we will need to define what counts as a \"chunk\" in this file, so when the\n",
    "    # Dataloader is loading (iterating over) the file and feeding it to the embedding\n",
    "    # model, it knows what to treat as one unit. Here, we (arbitrarily) say that one\n",
    "    # line in the file (corresponding to a paragraph) is one chunk.\n",
    "    def __iter__(self):\n",
    "        for line in open(self.corpus, \"r\", encoding=\"utf-8\"): # lines are split by \"\\n\"\n",
    "            # check that the line is not empty (if it is, do nothing):\n",
    "            if line == None:\n",
    "                continue\n",
    "            # tokenize and lowercase the string before yielding it\n",
    "            encoded_text=[]\n",
    "            tokens=word_tokenize(line)\n",
    "            for t in tokens:\n",
    "                index=t.lower()\n",
    "                encoded_text.append(index)\n",
    "            # the output must be a list of tokens in the line\n",
    "            yield encoded_text\n",
    "\n",
    "# Loading and cleaning the data\n",
    "path=\"dataset\"\n",
    "speeches_test=MyDataLoader(path+\"/allspeeches_77_2022.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
