{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Social Data Science 2\n",
    "\n",
    "*By Carl, Asger, & Esben*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import gensim.downloader\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "# We also set a random state in order to make reproducible results\n",
    "RANDOM_STATE = 1111\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Supervised text classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the training data set to fit at least one of the following models to try and predict the hate speech status of the tweets, using TF-IDF features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the `tweet_eval` dataset, split into train, validation, and test. `0 = non-hate` and `1 = hate`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (C:/Users/45242/.cache/huggingface/datasets/tweet_eval/hate/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Found cached dataset tweet_eval (C:/Users/45242/.cache/huggingface/datasets/tweet_eval/hate/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Found cached dataset tweet_eval (C:/Users/45242/.cache/huggingface/datasets/tweet_eval/hate/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"tweet_eval\", \"hate\", split = \"train\")\n",
    "val_data = load_dataset(\"tweet_eval\", \"hate\", split = \"validation\")\n",
    "test_data = load_dataset(\"tweet_eval\", \"hate\", split = \"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of observations within each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets in train: 9000\n",
      "Tweets in validation: 1000\n",
      "Tweets in test: 2970\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tweets in train: {train_data.shape[0]}\")\n",
    "print(f\"Tweets in validation: {val_data.shape[0]}\")\n",
    "print(f\"Tweets in test: {test_data.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text and labels into seperate lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [tweet[\"text\"] for tweet in train_data]\n",
    "train_labels = [tweet[\"label\"] for tweet in train_data] \n",
    "test_corpus = [tweet[\"text\"] for tweet in test_data]\n",
    "test_labels = [tweet[\"label\"] for tweet in test_data] \n",
    "val_corpus = [tweet[\"text\"] for tweet in val_data]\n",
    "val_labels = [tweet[\"label\"] for tweet in val_data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine class distribution in the different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribtion in train: {'non-hate': 5217, 'hate': 3783}\n",
      "Label distribtion in validataion: {'non-hate': 573, 'hate': 427}\n",
      "Label distribtion in test: {'non-hate': 1718, 'hate': 1252}\n"
     ]
    }
   ],
   "source": [
    "label_dict = {0:\"non-hate\", 1:\"hate\"}\n",
    "\n",
    "print(f\"Label distribtion in train: {dict(Counter([label_dict[label] for label in train_labels]))}\")\n",
    "print(f\"Label distribtion in validataion: {dict(Counter([label_dict[label] for label in val_labels]))}\")\n",
    "print(f\"Label distribtion in test: {dict(Counter([label_dict[label] for label in test_labels]))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing - to avoid an extensive process of providing accurate Part-of-Speech-tags to the lemmatizer, as well as the increased computational cost, we only stem the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(_str):\n",
    "    # remove twittertags + lowercase \n",
    "    _str = re.sub(r'@\\w+', \"\", _str.lower())\n",
    "    # remove numbers\n",
    "    _str = re.sub(r'\\d+', \"\", _str) \n",
    "    # remove punctuations \n",
    "    _str = _str.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"!\",\"\")))\n",
    "    # Remove extra whitespaces \n",
    "    _str = re.sub(r'\\s+', ' ', _str.strip())  \n",
    "    # tokenize text - we do not use TweetTokenize as we have remove @ either way\n",
    "    tokens = nltk.word_tokenize(_str)\n",
    "    # remove stopwords and stem\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    return ' '.join(tokens) # join words back into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_preproc = [preproc(tweet) for tweet in train_corpus]\n",
    "test_corpus_preproc = [preproc(tweet) for tweet in test_corpus]\n",
    "val_corpus_preproc = [preproc(tweet) for tweet in val_corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerations on the preprocessing**: As preprocessing is more of an assessment than a hard science, we have made some decision that we think are best suited for the task at hand, i.e. to classify hate-speech on twitter, some of these are:\n",
    "- Retaining `!` when removing punctiation, as exclamations could be useful for the model when predicting hate-speech. \n",
    "- Though capital letters could convey some meaning when predicting hate-speech, We have lower-cased all words, to make the classification more about semnatic meaning than letter-capitalization.  \n",
    "\n",
    "Ultimalty, how one preprocesses data is about how to convey relevant nuances in the data, in the most simplified way, e.g. by removing stopwords, if these do not provide any information to the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `TfidfVectorizer()` to convert the documents into a TF-IDF feature matrix. We use both `unigrams` and `bigrams` to give the model a little more context about the context of the words - i.e. neighboring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=\"word\",\n",
    "                             ngram_range = (1,2))\n",
    "                             \n",
    "train_features = vectorizer.fit_transform(train_corpus)\n",
    "# only transform() on val and test, to make the evaluation resemeble \"unseen data\" more\n",
    "val_features = vectorizer.transform(val_corpus)  \n",
    "test_features = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with ridge regularization (with a c parameter tuned via cross-validation on the train set)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the hyperparameter, `C`, for regularizing the model to avoid overfitting, we carry out a `GridSearchCV`, with 5 folds and spanning `np.logspace(0, 1, 30)` values of `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.77\n",
      "Best parameters:  {'C': 10.722672220103236}\n",
      "Best estimator:  LogisticRegression(C=10.722672220103236, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"C\": np.logspace(-2, 2, 100)} # search from 10^-2 to 10^2 with 100 steps\n",
    "\n",
    "lr_grid = GridSearchCV(LogisticRegression(penalty=\"l2\", random_state=0), \n",
    "                        param_grid = param_grid, \n",
    "                        cv=5, \n",
    "                        n_jobs=-1, \n",
    "                        scoring = \"accuracy\")\n",
    "\n",
    "lr_grid.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(lr_grid.best_score_))\n",
    "print(\"Best parameters: \", lr_grid.best_params_)\n",
    "print(\"Best estimator: \", lr_grid.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with a linear kernel (with a c parameter tuned via crossvalidation on the train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.77\n",
      "Best parameters:  {'C': 1.0974987654930561}\n",
      "Best estimator:  LinearSVC(C=1.0974987654930561, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"C\": np.logspace(0, 2, 100)} # search from 10^-10 to 10^2 with 30 steps\n",
    "\n",
    "lscv_grid = GridSearchCV(LinearSVC(penalty=\"l2\", random_state=0), \n",
    "                        param_grid = param_grid, \n",
    "                        cv=5, \n",
    "                        n_jobs=-1, \n",
    "                        scoring = \"accuracy\")\n",
    "\n",
    "lscv_grid.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(lscv_grid.best_score_))\n",
    "print(\"Best parameters: \", lscv_grid.best_params_)\n",
    "print(\"Best estimator: \", lscv_grid.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes does not take the `TfidfVectorizer()` as input, but rather `CountVectorizer()`, i.e. simply the word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   2.5s\n",
      "[CV] END .................................................... total time=   2.5s\n",
      "[CV] END .................................................... total time=   2.5s\n",
      "[CV] END .................................................... total time=   2.6s\n",
      "Naive Bayes cross-validation accuracy:  0.7411111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   13.5s finished\n"
     ]
    }
   ],
   "source": [
    "counterizer = CountVectorizer()\n",
    "train_counts = counterizer.fit_transform(train_corpus)\n",
    "val_counts = counterizer.transform(val_corpus)\n",
    "test_counts = counterizer.transform(test_corpus)\n",
    "\n",
    "nb_score = cross_val_score(MultinomialNB(), \n",
    "                           train_counts.toarray(), \n",
    "                           train_labels, \n",
    "                           cv=5, \n",
    "                           scoring=\"accuracy\", \n",
    "                           verbose=2)\n",
    "\n",
    "print(\"Naive Bayes cross-validation accuracy: \", np.mean(nb_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also fit at least one of the following models to try and predict the hate speech status of the tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the following models take and *\"understands\"* sequential data inputs, as opposed to the previous models, who use a bag-of-words or Tf-IDF matrix as inputs, where the order the the words in the sentence are neglected. Thus, we have to preprocess the input data differently. For instance, the word `not` is part of nltk's `stopwords.words('english')`-package. The word `not` can shift the meaning of a sentence completly, depending on it's position. Non-sequential datastructures and models have no way of decerning which part of the sentece the `not` relates to - but sequential models do. Thus, we will commit a much less extensive data cleaning process in the preprocessing phase, and only lower-case. `Stemming` and `Lemmatizing` would also reduce the semantic meaning of the words, so this will not commit either of these steps as well. BLABLA, måske en henvisning til noget.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"not\" in stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to represent each token through an embedding, rather than as a Tf-Idf-score or word-count, we use the pretrained embeddings from `glove-twitter-200` to embed the features in our vocabulary into a 200-dimensional vector space. One of the main advantages of this kind of text `vectorization` - e.g. compared to one-hot encoding - is to reduced the dimensionality of the feature space and decrease the effect of the curse of dimensionality (Raschka & Mirjalili 2019:590). With TF-Idf, the feature matrix had the dimension `vocabulary_length x n_documents` - 88.987 x 9.000 in the above models, where both unigrams and bigrams were included - and was a sparse matrix mostly containing zeros. This is not the case with embeddings, where we reduce the dimenensions of the feature matrix to `vocabulary_length x embedding_dimension` (20.620 x 200).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained embeddings (these can be used as the embedding argument in create_embedding_matrix)\n",
    "glove = gensim.downloader.load('glove-twitter-200')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to establish our total vocabulary. We lowercase all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the full list of vocabulary in the tweet_eval data\n",
    "total_vocabulary = set()\n",
    "for tweet in train_corpus + val_corpus+test_corpus:\n",
    "    tokens = word_tokenize(tweet)\n",
    "    for t in tokens:\n",
    "        total_vocabulary.add(t.lower()) # lower.case\n",
    "total_vocabulary = sorted(list(total_vocabulary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include a padding-token, \"\", as the first entry, which will be the first row in the embedding matrix. We are going to ensure that the input sequences - i.e. tokenized sentences - have the same length, which we will do by continously appending empty string token to the front of the list of tokens in sentences that are shorter than the longest sentece. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending an empty padding token at the beginning of the vocabulary\n",
    "total_vocabulary = [\"\"]+total_vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a embedding matrix with the dimension `vocabulary_length x embedding_dimension`. These 200-dimensional vector will serve as representations of the features and be the input to the RNN and LSTM. Tokens that are not already in the pretrained embeddings will have all their values set to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24019/24019 [00:00<00:00, 161210.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.1390507910075 % of tokens are out of vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(tokens, embedding):\n",
    "    \"\"\"creates an embedding matrix from pre-trained embeddings for a new vocabulary. It also adds an extra vector\n",
    "    of zeroes in row 0 to embed the padding token, and initializes missing tokens as vectors of 0s\"\"\"\n",
    "    oov = set()\n",
    "    size = embedding.vector_size\n",
    "    # note the extra zero vector that will used for padding\n",
    "    embedding_matrix=np.zeros((len(tokens),size))\n",
    "    c = 0\n",
    "    for i in tqdm(range(1,len(tokens))):\n",
    "        try:\n",
    "            embedding_matrix[i]=embedding[tokens[i]]\n",
    "        except KeyError: #to catch the words missing in the embeddings\n",
    "            try:\n",
    "                embedding_matrix[i]=embedding[tokens[i].lower()]\n",
    "            except KeyError:\n",
    "                #if the token does not have an embedding, we initialize it as a vector of 0s\n",
    "                embedding_matrix[i] = np.zeros(size)\n",
    "                #we keep track of the out of vocabulary tokens\n",
    "                oov.add(tokens[i])\n",
    "                c +=1\n",
    "    print(f'{c/len(tokens)*100} % of tokens are out of vocabulary')\n",
    "    return embedding_matrix, oov\n",
    "\n",
    "#get the embedding matrix and out of vocabulary words for our tweet_eval vocabulary\n",
    "embedding_matrix, oov = create_embedding_matrix(total_vocabulary, glove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text sentences into a list of indicies, each of which corresponding to the words' indices in the total vocabulary and in the embedding matrix. Also, we use padding on the feature vectore to ensure that all vector respresentations of the input sentences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(text, total_vocabulary):\n",
    "    \"\"\"turns the input text (one tweet) into a vector of indices in total_vocabulary that corresponds to the tokenized words in the input text\"\"\"\n",
    "    encoded_text = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for t in tokens:\n",
    "        index = total_vocabulary.index(t.lower())\n",
    "        encoded_text.append(index)\n",
    "    return encoded_text\n",
    "\n",
    "def add_padding(vector, max_length, padding_index):\n",
    "    \"\"\"adds copies of the padding token to make the input vector the max_length size, so that all inputs are the same length\n",
    "    (the length of tweet with most words)\"\"\"\n",
    "    if len(vector) < max_length:\n",
    "        vector = [padding_index for _ in range(max_length-len(vector))] + vector\n",
    "    return vector\n",
    "\n",
    "# vectorize sentences to indices\n",
    "train_features = [text_to_indices(text, total_vocabulary) for text in train_corpus]\n",
    "val_features = [text_to_indices(text, total_vocabulary) for text in val_corpus]\n",
    "test_features = [text_to_indices(text, total_vocabulary) for text in test_corpus]\n",
    "\n",
    "# calc length of longest tweet\n",
    "longest_tweet = max(train_features+val_features, key=len)\n",
    "max_length = len(longest_tweet)\n",
    "padding_index = 0 #position 0 is where we had put the padding token in our vocabulary and embedding matrix\n",
    "\n",
    "# padding the feature vectors\n",
    "train_features = [add_padding(vector, max_length, padding_index) for vector in train_features]\n",
    "val_features = [add_padding(vector, max_length, padding_index) for vector in val_features]\n",
    "test_features = [text_to_indices(text, total_vocabulary) for text in test_corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert all splits of the data into a PyTorch DataSet as well as a DataLoader, to perform `batch gradient descent` when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetEvalTrain(torch.utils.data.Dataset):\n",
    "    # defining the sources of the data\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = torch.LongTensor(features)\n",
    "        self.y = torch.from_numpy(np.array(labels))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index].unsqueeze(0)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "data_train = TweetEvalTrain(train_features, train_labels)\n",
    "data_val = TweetEvalTrain(val_features, val_labels)\n",
    "data_test = TweetEvalTrain(val_features, val_labels) \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=64)\n",
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data is all set to be fed into the RNN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-one RNN with pre-trained word embeddings as the inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define both a custom RNN/LSTM class, a training_loop function and an evaluation function. Note that we do not need to explicitly define a softmax-activation for the outputs to be passed through, as our loss function, `CrossEntropyLoss`, automatically applies a softmax transformation before calculating the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced version supporting multiple types of RNN layers\n",
    "\n",
    "class RNN_or_LSTM(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix, type=\"RNN\"):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n",
    "        emb_dim = embedding_matrix.shape[1] # = 200 in this case --> Det er vores input_size, og representere blot 1 ord, men gennem 200 features! Derfor er der også 200 vægte mellem input-layer og hver node i hidden layer\n",
    "        \n",
    "        #remember the batch_first=True argument\n",
    "        if type == \"RNN\":\n",
    "            self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=1, batch_first=True)\n",
    "        elif type == \"LSTM\":\n",
    "            self.rnn = torch.nn.LSTM(input_size=emb_dim, hidden_size=rnn_size, bidirectional=False, num_layers=1, batch_first=True)\n",
    "        else:\n",
    "            raise LookupError(\"Only RNN and LSTM are supported.\")\n",
    "        self.linear_output = torch.nn.Linear(rnn_size, n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "\n",
    "        #apply the RNN or LSTM\n",
    "        if type == \"RNN\":\n",
    "            all_states, final_state = self.rnn(encoded_inputs)\n",
    "        else:\n",
    "            # LSTM's output is different and needs to be treated differently, see documentation for details\n",
    "            all_states, (final_state, c_n) = self.rnn(encoded_inputs)\n",
    "        \n",
    "        # run the final states through the output layer\n",
    "        outputs = self.linear_output(final_state.squeeze())\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our RNN and LSTM classes, where the number of hidden layers can be change using the `num_layers` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the embedding step and RNN model\n",
    "\n",
    "class RNN_LSTM(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix, num_layers=1, type_=\"RNN\"):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True) # the tokens corresponding to the padding_idx will not be included in the training of the model \n",
    "        emb_dim = embedding_matrix.shape[1] # = 200 in this case --> It is our input_size, and it represents just one word, but through 200 features! That's why there are also 200 weights between the input layer and each node in the hidden layer.\n",
    "        self.num_layers = num_layers\n",
    "        self.type = type_ \n",
    "\n",
    "        #remember the batch_first=True argument\n",
    "        if self.type == \"RNN\":\n",
    "            self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=self.num_layers, batch_first=True)\n",
    "        elif self.type == \"LSTM\":\n",
    "            self.rnn = torch.nn.LSTM(input_size=emb_dim, hidden_size=rnn_size, bidirectional=False, num_layers=self.num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise LookupError(\"Only RNN and LSTM are supported.\")\n",
    "\n",
    "        #define the output layer (no softmax needed here; we will apply softmax as part of the loss calculation)\n",
    "        #applies a linear transformation to the RNN\n",
    "        #final layer state and outputs scores for the n classes\n",
    "        self.linear_outputs = torch.nn.Linear(rnn_size, n_classes)                      \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "\n",
    "        # The RNN returns two tensors: one representing the hidden states at all positions,\n",
    "        # and another representing only the final hidden states.\n",
    "        # In this many-to-one model, we only need the final hidden states,\n",
    "        # i.e the prediction at the end of the sequence, where all prio information is somehow present\n",
    "        \n",
    "        # run the final state through the output layer\n",
    "        if self.type == \"RNN\" and self.num_layers > 1:\n",
    "            all_states, final_state = self.rnn(encoded_inputs)\n",
    "            outputs = self.linear_outputs(final_state[-1,:,:]) # only activation from the last layer in the stack of layers\n",
    "\n",
    "        elif self.type == \"RNN\" and self.num_layers == 1:\n",
    "            all_states, final_state = self.rnn(encoded_inputs)\n",
    "            final_state = final_state.squeeze()\n",
    "            outputs = self.linear_outputs(final_state)\n",
    "\n",
    "        elif self.type == \"LSTM\" and self.num_layers > 1:\n",
    "            all_states, (final_state, c_n) = self.rnn(encoded_inputs)\n",
    "            outputs = self.linear_outputs(final_state[-1,:,:]) # only activation from the last layer in the stack of layers\n",
    "            \n",
    "        elif self.type == \"LSTM\" and self.num_layers == 1:\n",
    "            # LSTM's output is different and needs to be treated differently, see documentation for details\n",
    "            all_states, (final_state, c_n) = self.rnn(encoded_inputs)\n",
    "            outputs = self.linear_outputs(final_state) \n",
    "\n",
    "        return outputs\n",
    "\n",
    "# training loop\n",
    "def training_loop(model, num_epochs):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad() # zero the gradients that are stored from the previous optimization step\n",
    "            outputs = model.forward(inputs).squeeze() # compute the outputs of the model\n",
    "            targets = targets.squeeze().long() #dependending on your torch version you might have to use targets = targets.squeeze().long()\n",
    "            loss = loss_function(outputs, targets) #loss function (Adam) compares model output and the true labels\n",
    "            loss.backward() # Backpropagation -> get derivative of loss function \n",
    "            optimizer.step() # optimize based on derivative\n",
    "            losses.append(loss.item()) #add this batch's loss to the losses for this epoch\n",
    "        print(f'Epoch {epoch+1}: loss {np.mean(losses)}')\n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad(): # for evaluation we don't backpropagate and update weights anymore\n",
    "        for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "            outputs = torch.softmax(model(inputs), 1 ) # apply softmax to get probabilities/logits\n",
    "            # getting the indices of the logit with the highest value, which corresponds to the predicted class (as labels 0, 1, 2)\n",
    "            vals, indices = torch.max(outputs, 1)\n",
    "            # accumulating the predictions\n",
    "            predictions += indices.tolist()\n",
    "            # accumulating the true labels\n",
    "            labels += targets.tolist()\n",
    "    \n",
    "    acc = accuracy_score(predictions, labels)\n",
    "    print(f'Model accuracy: {acc}')\n",
    "    return acc, predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train an RNN with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.6019936258911242\n",
      "Epoch 2: loss 0.5577724246268577\n",
      "Epoch 3: loss 0.532719999974501\n",
      "Epoch 4: loss 0.512544610821609\n",
      "Epoch 5: loss 0.5200163743174668\n",
      "Model accuracy: 0.674\n"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 1, embedding_matrix=embedding_matrix)\n",
    "\n",
    "myRNN = training_loop(myRNN, num_epochs = 5)\n",
    "acc, preds = evaluate(myRNN, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a Deep RNN with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[189], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# initializing and training the model:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m myRNN \u001b[39m=\u001b[39m RNN_LSTM(rnn_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, n_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, num_layers \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, embedding_matrix\u001b[39m=\u001b[39membedding_matrix)\n\u001b[1;32m----> 4\u001b[0m myRNN \u001b[39m=\u001b[39m training_loop(myRNN, num_epochs \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m acc, preds \u001b[39m=\u001b[39m evaluate(myRNN, val_loader)\n",
      "Cell \u001b[1;32mIn[181], line 68\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, num_epochs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m batch_index, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     67\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# zero the gradients that are stored from the previous optimization step\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(inputs)\u001b[39m.\u001b[39;49msqueeze() \u001b[39m# compute the outputs of the model\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mlong() \u001b[39m#dependending on your torch version you might have to use targets = targets.squeeze().long()\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(outputs, targets) \u001b[39m#loss function (Adam) compares model output and the true labels\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "myRNN = training_loop(myRNN, num_epochs = 5)\n",
    "acc, preds = evaluate(myRNN, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.603381429369568\n",
      "Epoch 2: loss 0.5530780082476054\n",
      "Epoch 3: loss 0.5255337934544746\n",
      "Epoch 4: loss 0.5062031760706124\n",
      "Epoch 5: loss 0.49184963931428627\n",
      "Model accuracy: 0.692\n"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "myRNN = training_loop(myRNN, num_epochs = 5)\n",
    "acc, preds = evaluate(myRNN, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m myLSTM \u001b[39m=\u001b[39m RNN_LSTM(rnn_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, n_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, num_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, embedding_matrix\u001b[39m=\u001b[39membedding_matrix, type_\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLSTM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m myLSTM \u001b[39m=\u001b[39m training_loop(myLSTM, num_epochs \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m acc, preds \u001b[39m=\u001b[39m evaluate(myLSTM, val_loader)\n",
      "Cell \u001b[1;32mIn[181], line 68\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, num_epochs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m batch_index, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     67\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# zero the gradients that are stored from the previous optimization step\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(inputs)\u001b[39m.\u001b[39;49msqueeze() \u001b[39m# compute the outputs of the model\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mlong() \u001b[39m#dependending on your torch version you might have to use targets = targets.squeeze().long()\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(outputs, targets) \u001b[39m#loss function (Adam) compares model output and the true labels\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "myLSTM = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 1, embedding_matrix=embedding_matrix, type_=\"LSTM\")\n",
    "\n",
    "myLSTM = training_loop(myLSTM, num_epochs = 3)\n",
    "acc, preds = evaluate(myLSTM, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "myLSTM = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 1, embedding_matrix=embedding_matrix, type_=\"LSTM\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myRNN = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 2, embedding_matrix=embedding_matrix)\n",
    "\n",
    "myRNN = training_loop(myRNN, num_epochs = 5)\n",
    "acc, preds = evaluate(myRNN, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fine-tuned BERT model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each of the models you ran in question 1. and 2., briefly discuss (two–four sentences) in what ways the model is a good choice for the current task and data set, plus any downsides the model might have for this application.\n",
    "\n",
    "\n",
    "When using pretrained embeddings for text vectorization, the corpus used to construct n-dimensional vector space is very important for the performance of our model, for instance in relation to inherent biases in the embeddings stemming from the data on which it was trained, or if the vocabulary of the embeddings is small, wherefor many of the tokens from our own dataset might not be embed-able as they are not part of the embeddings vocabulary. We saw that around 30% of the tokens were not in the pretrained vocabulary, so almost a third of our vocabulary will not be used to train the model or when predicting. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Present the performance of your models on the test set in terms of both F1 and accuracy. Which is the best-performing model in terms of F1? In your opinion, should we prefer F1 over accuracy as an indicator of model performance here, and why?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the paper by Basile et al. (2019), What further info might you have liked to have about the data selection process and/or the annotation process for this data set? Why?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam-asds-0J4d57eE-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
