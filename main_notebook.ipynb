{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"LUtlvdadkS0D"},"source":["# Advanced Social Data Science 2\n","\n","*By Carl, Asger, & Esben*"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Part 1 - Measuring grandstanding in parliaments"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In the article, the authors investigate when and why politicians use emotive rhetoric in the legislative arena. They argue that politicians are more inclined to employ such rhetoric when their speeches are delivered to a large general audience, using it as a strategic tool to appeal to voters. Consistent with their expectations, they find that politicians use emotive rhetoric more in high-profile legislative debates (Osnabrügge et al. 2021: 885). In the following sections, we will assess their choice of methods for quantifying emotive rhetoric in parliamentary speeches. First, we outline how the authors justify their choice of a dictionary-based approach and discuss its limitations in this context. Subsequently, we turn to how the authors use word embeddings to augment the ANEW dictionary with domain specificity, thereby addressing and rectifying some of the shortcomings associated with relying solely on a dictionary-based approach. Lastly, we present and discuss alternative approaches. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## How do OHR justify their dictionary-based approach and what are its weaknesses in this context?\n","In order to investigate when politicians use emotive rhetoric, it is essential for the authors to establish an accurate measure. To achieve this, OHR utilizes the ANEW dictionary in combination with word-embedding techniques to take a dictionary-based approach, i.e. every word in the text corpus is mapped to a dictionary that contains a specific emotive value for the word. One of the main reasons that the authors chose a dictionary-based method is that it is cost-effective. The alternative of using supervised learning, where humans hand-annotate speeches based on their level of emotiveness, can be quite costly. In contrast, the ANEW dictionary is already constructed, freely available, and well-established within the scientific community. It also facilitates the application of word-embedding techniques due to the inclusion of complete words, and the fact that it is free allows for replication (Osnabrügge et al. 2021: 890). \n","\n","The authors also justify their approach by stating that the ANEW dictionary is “not contaminated by partisan attitudes and political predispositions” (Osnabrügge et al. 2021: 890). However, going a bit more into the context of ANEW’s construction, it can be questioned whether the dictionary is completely free from latent - possibly political - biases. The ratings in ANEW were acquired using an affective rating system where subjects were asked to rate words on the dimensions of pleasure, arousal, and dominance (Bradley & Lang 1999: 1). The respondents were “Introductory Psychology Class” students who participated as part of a course requirement (Bradley & Lang 1999: 2). Though they were balanced on gender, it can still be questioned whether psychology students represent an unbiased social group of annotators or if some form of annotation bias is embedded in the ANEW dictionary (Hovy 2022: 20). \n","\n","Relying on ANEW for a dictionary-based approach in a political context, it is worth noting that ANEW is not designed with the political context in mind. This leads to two related challenges: The first is that by using a dictionary constructed in a different context, we risk assigning emotive values to words that may not be considered equally emotive in the political context. The second is that we - if we solely rely on the ANEW dictionary - risk excluding relevant emotive words from the political context, as they are not part of the dictionary’s vocabulary. As the next section (1.2) will show, the authors solves the latter problem by using word-embedding techniques. \n","\n","However, the former problem with context specificity remains unsolved and is not a neglectable issue. Delatorre et al. (2019: 1) demonstrate in their study of the ANEW dictionary, that the affective ratings of words are highly influenced by the context they appear in. Thus, words that were originally considered emotive by the annotators are not necessarily considered emotive when they appear within the political arena. Consequently, as the authors also state in the article, we run the risk of generating measurement errors (Osnabrügge et al. 2021: 890). \n","\n","To Delatorre et al. (2019), the lack-of-context issue is a theoretical consideration, pertaining to the varying connotative interpretation of words within different social arenas. However, it also presents a more methodological issue. Dictionary-based approaches disregard the sentence-level context of the words in its vocabulary. In ANEW, the particular emotive value of a word is constant across all imaginable sentences and contexts. In section 1.3, we will discuss alternative methodological approaches that address this weakness of the dictionary-based approach.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## How do OHR augment the ANEW dictionary, and how does this contribute to domain specificity? \n","The authors augment the ANEW dictionary by leveraging word-embedding techniques, thereby adding context-specific out-of-vocabulary words to the ANEW dictionary along with an emotive score.\n","\n","First, they identify so-called seed words from the ANEW dictionary, i.e. words that appear in both ANEW and the parliamentary speeches and have a sufficiently high emotive or neutral score with a relatively small standard deviation. Next, OHR uses a skip-gram model with hierarchical softmax to construct word embeddings, thus vectorizing all words in their corpus. They can then calculate an emotive score for each non-seed word - i.e. corpus words that do not appear in the ANEW dictionary - based on their cosine-similarity to seed words in the embedding space. To calculate an emotive score, the sum of a non-seed word’s cosine-similarity with all neutral seed words is subtracted from the sum of the word’s cosine-similarity with all emotive seed words. Put simply, they calculate whether a word is positioned generally closer to emotive words or neutral words in the embedding space and how much closer. They add the non-seed words scoring above the 0.975 and under the 0.025 quantile to ANEW - that is, the most emotive and most neutral words, respectively. This expands the vocabulary of ANEW to comprise 2.015 emotive words and 2.095 neutral words. The exact emotive/neutral values of the newly added vocabulary words are less important, as OHR later determine the degree of emotion in a speech by simply subtracting the percentage of neutral from the percentage of emotive words.\n","\n","In OHR’s own words, the utilization of this embedding method makes their approach more “domain-specific”, as they can estimate an emotive score of words that appear in the political domain of the parliament speeches but have not been annotated in the default ANEW (Osnabrügge et al. 2021: 890). Thus, they are able to extract more information and capture a broader range of emotive nuances from the parliamentary speeches, while still retaining a dictionary-based approach - even without having to construct the dictionary from scratch themselves.\n","\n","This approach solves one of the two dictionary-based problems described in the previous section, namely the risk of excluding relevant emotive information about the out-of-vocabulary words appearing in the political context. However, we are still left with the risk that the emotive and neutral words in ANEW have different connotations when they are contextualized in a political setting or depending on the semantic context of the sentences they appear in. Downstream, this concern would also affect the embedding-based augmentation of the ANEW dictionary, as these would inherit the lack of political context when they are assigned an emotive score based on the “contextless” ANEW words. In the next section, we address alternative approaches to this potential issue.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Alternative approaches and their main advantages and disadvantages\n","We have mainly criticized the approach of OHR for the lack of context associated with their dictionary-based approach. Here, we present two alternative approaches to potentially counter this issue.\n","\n","One alternative approach to measure emotive rhetoric in parliaments, which actually captures the semantic context of the words, could be to fine-tune a BERT model that has been pre-trained on parliament speeches for domain specificity. One of the main advantages of using BERT is its ability to handle sequential data inputs, thus allowing it to understand the context of words or phrases within the given sentences. At the core of BERT’s context awareness is the “self-attention” mechanism from the Transformer architecture, entailing multi-head attention and Query-Key-Value parameterization (Raschka & Mirjalili 2019: 616). Taking a bidirectional approach, this architecture allows BERT to pay attention to other parts of a sentence - both left and right - as it processes each individual word. Thus, a word deemed emotive in one sentence might not be regarded as emotive in another. Though this method would provide a much more nuanced understanding of the emotiveness of speeches, there are two main caveats to this approach. First, OHR would either need to find a BERT model that is already pre-trained on text derived from the political context or pre-train the BERT themselves, which requires a lot of data, is computationally expensive, and can be rather time consuming (Bender et al. 2021: 612). Having found or pre-trained a BERT, it would still need fine-tuning for the task it needs to perform. In OHR, the authors “compute the level of emotive rhetoric by subtracting the percentage of neutral from the percentage of emotive words” (Osnabrügge et al. 2021: 891). Using BERT, one could take a classification approach, where the model would assign either a binary or score-based value to entire sentences, paragraphs, or speeches, circumventing the count-words-and-summate measure in OHR. This leads us to the second caveat. However one defines the classification task of the BERT, fine-tuning would require data annotation that the model can be trained on for task specificity. As OHR mentioned in their article, this can be costly. \n","\n","A second alternative approach, which could work as a supplementary contextualization of the emotive words in OHR’s dictionary-based approach, would be to include audio and/or image data. Knox & Lucas (2021) have criticized the extensive focus on word usage rather than word delivery in political studies, advocating for audio-as-data approaches. Relevant to OHR’s study of emotive rhetoric in parliaments, Dietrich et al. (2019) have constructed a measure for emotional intensity among legislators in Congressional floor speeches based on vocal pitch. They present their work as a demonstration that audio-as-data can be both feasible and informative when studying political speeches. OHR could combine such vocal pitch measures of emotional intensity to scale the emotive scores in their augmented ANEW dictionary, based on how the word was vocally delivered. Turning to image/video data, OHR could further enrich their measure of emotive rhetoric by looking at images of the ones who utter an emotive or neutral ANEW word, when they deliver the words. Facial expression and body language could both carry relevant information regarding the degree of emotiveness associated with the utterance of a word. The conventional approach to image-as-data tasks within the social sciences is to use convolutional neural networks (Torres & Francisco 2022, Williams et al. 2020). As with audio, OHR could utilize image classification to contextualize and scale the emotive scores in their augmented ANEW dictionary.\n","\n","Naturally, the inclusion of either audio or images would entail a more extensive data collection process of gathering the possibly non-existing or sparse audio and video material from parliamentary speeches over time. Furthermore, they would have to familiarize themselves with entirely new modeling approaches, matching and applying Dietrich et al.'s (2019) emotional intensity measure and implementing convolutional neural networks for image classification. Also, a caveat working with these two data types is that both are computational demanding data, especially regarding data storage (Rheault 2022: 2; Williams 2020: 13; Torres 2022: 123). Finally, both audio and especially images entail privacy concerns, which is important to be fully aware of before processing the data (Williams 2020: 14).\n","\n","As OHR have already expressed their concern toward the cost of annotating data for supervised learning approaches, neither of the above approaches seem within the scope of OHR’s design. However, both approaches - BERT and audiovisual data - would most likely yield more accurate representations of the degree of emotive rhetoric in parliament speeches.  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1686728066849,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"bJdrKTFxksPS"},"outputs":[],"source":["# !pip install datasets==2.2.1 transformers==4.19.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17580,"status":"ok","timestamp":1686739356555,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"HGlKMLJVkS0J","outputId":"55f81027-e072-4168-c1c5-ec47086beb69"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import string\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, f1_score\n","import gensim.downloader\n","import torch\n","import re\n","from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n","from datasets import load_metric, load_dataset\n","import numpy as np\n","from google.colab import drive\n","import nltk\n","import pandas as pd\n","import gensim.downloader\n","import pandas as pd\n","import numpy as np\n","import torch\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aLNZQNEBkS0L"},"source":["# Part 3 Supervised text classification\n","\n","In this section, we will train a range of models to predict whether a tweet could be classified as hate speech or not. We will use a ridge regularized Logistic Regression and two BERT models with different pre-trainings - a general purpose BERT and a BERT specialized to detect hate speech on Twitter.  "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zAkxIs8JkS0M"},"source":["## Fitting a logistic regression to predict the hate speech status of the tweets, using TF-IDF features."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p5AkzQcGkS0N"},"source":["First, we load the `tweet_eval` dataset and split the data into train, validation, and test. In the data `0` signifies non-hate and `1` indicates hate."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1352,"status":"ok","timestamp":1686743788988,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"bZRqtUdJkS0O","outputId":"feb0db19-b532-486b-bb02-bf0da9b08bd8"},"outputs":[],"source":["train_data=load_dataset(\"tweet_eval\", \"hate\", split=\"train\")\n","val_data=load_dataset(\"tweet_eval\", \"hate\", split=\"validation\")\n","test_data=load_dataset(\"tweet_eval\", \"hate\", split=\"test\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GDarlbM0kS0S"},"source":["Split text and labels into seperate lists."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1367,"status":"ok","timestamp":1686743790354,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"dAkT-zQtkS0S"},"outputs":[],"source":["train_corpus=[tweet[\"text\"] for tweet in train_data]\n","train_labels=[tweet[\"label\"] for tweet in train_data]\n","test_corpus=[tweet[\"text\"] for tweet in test_data]\n","test_labels=[tweet[\"label\"] for tweet in test_data]\n","val_corpus=[tweet[\"text\"] for tweet in val_data]\n","val_labels=[tweet[\"label\"] for tweet in val_data]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HgyV6FT-kS0T"},"source":["Examine number of observations and class distribution in the different splits."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686743790355,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"K0G0-CpokS0U","outputId":"f1feb449-d48f-4369-bf30-222198ab736d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Label distribtion in train: {'non-hate': 5217, 'hate': 3783}\n","Label distribtion in validataion: {'non-hate': 573, 'hate': 427}\n","Label distribtion in test: {'non-hate': 1718, 'hate': 1252}\n"]}],"source":["label_dict={0:\"non-hate\", 1:\"hate\"}\n","\n","print(f\"Label distribtion in train: {dict(Counter([label_dict[label] for label in train_labels]))}\")\n","print(f\"Label distribtion in validataion: {dict(Counter([label_dict[label] for label in val_labels]))}\")\n","print(f\"Label distribtion in test: {dict(Counter([label_dict[label] for label in test_labels]))}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0--LLK_mkS0U"},"source":["Preprocessing the data. The authors of the `TweetEval Hate Speech` dataset have already done a bit of text preprocessing, converting username, indicated by a leading `@`, into a standard `@username` token and convert all URLs, indicated by a leading `http` into a standard `http` token."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686743790355,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"VEnddNPJkS0V"},"outputs":[],"source":["def preproc(_str):\n","    # remove twittertags + lowercase\n","    _str=re.sub(r'@\\w+', \"\", _str.lower())\n","    # remove numbers\n","    _str=re.sub(r'\\d+', \"\", _str)\n","    # remove punctuations\n","    _str=_str.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"!\",\"\")))\n","    # Remove extra whitespaces\n","    _str=re.sub(r'\\s+', ' ', _str.strip())\n","    # tokenize text - we do not use TweetTokenize as we have removed @ either way\n","    tokens=word_tokenize(_str)\n","    # remove stopwords and stem\n","    stemmer=PorterStemmer()\n","    tokens=[stemmer.stem(word) for word in tokens if word not in stopwords.words('english')]\n","\n","    return ' '.join(tokens) # join words back into a string"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":30957,"status":"ok","timestamp":1686743821309,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"ZqSCOtFekS0V"},"outputs":[],"source":["train_corpus_preproc=[preproc(tweet) for tweet in train_corpus]\n","test_corpus_preproc=[preproc(tweet) for tweet in test_corpus]\n","val_corpus_preproc=[preproc(tweet) for tweet in val_corpus]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8cJIW1O_kS0W"},"source":["As preprocessing is more of an assessment than a hard science, we have made some decisions that we think are best suited for the task at hand, i.e. to classify hate-speech on twitter. Some of these are:\n","- Retaining `!` when removing punctuation, as exclamations could be useful for the model when predicting hate-speech.\n","- Though capital letters could convey some meaning when predicting hate-speech, We have lower-cased all words, to make the classification more about semantic meaning than letter-capitalization.\n","- We only stem the tokens, though lemmatization arguably might improve model performance with proper Part-of-speech tags.\n","\n","Ultimately, preprocessing data is about conveying as many relevant nuances as possible in the most simplified way. For instance, if stopwords do not provide any information on hate speech to the model, they should be removed. Should the model be used in production or published, we would have engaged in a process of trial-and-error, going back and forth between different preprocessing steps to see which preprocessing yields the best model performance."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"io2za-w9kS0W"},"source":["Convert the documents into a TF-IDF feature matrix. We use both `unigrams` and `bigrams` to give the model a little more context about the context of the words - i.e. neighboring words."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":984,"status":"ok","timestamp":1686743822283,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"vmquDJgRkS0X"},"outputs":[],"source":["vectorizer=TfidfVectorizer(analyzer=\"word\",\n","                             ngram_range=(1,2))\n","\n","train_features_lr=vectorizer.fit_transform(train_corpus_preproc)\n","# only transform() on val and test, to make the evaluation resemble \"unseen data\" more\n","val_features_lr=vectorizer.transform(val_corpus_preproc)\n","test_features_lr=vectorizer.transform(test_corpus_preproc)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CqsFF42fkS0Y"},"source":["### Logistic regression with ridge regularization\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7RwUpuJekS0Y"},"source":["To optimize the hyperparameter - `C` - for regularizing the model to avoid overfitting, we carry out a `GridSearchCV`, with 5 folds and spanning `np.logspace(-2, 2, 50)` values of `C`. We use ridge-regression for regularization."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243848,"status":"ok","timestamp":1686744066129,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"YUJY3gFRkS0Y","outputId":"0eb2043a-940a-4be8-db7f-cd484b730d86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best cross-validation score: 0.77\n","Best parameters:  {'C': 39.06939937054613}\n","Best estimator:  LogisticRegression(C=39.06939937054613, max_iter=300, random_state=0)\n"]}],"source":["param_grid={\"C\": np.logspace(-2, 2, 50)} # search parameters to optimize\n","\n","lr_grid=GridSearchCV(LogisticRegression(penalty=\"l2\", random_state=0, max_iter=300),\n","                        param_grid=param_grid,\n","                        cv=5,\n","                        n_jobs=-1,\n","                        scoring=\"accuracy\")\n","\n","lr_grid.fit(train_features_lr, train_labels)\n","\n","print(\"Best cross-validation score: {:.2f}\".format(lr_grid.best_score_))\n","print(\"Best parameters: \", lr_grid.best_params_)\n","print(\"Best estimator: \", lr_grid.best_estimator_)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"28Hnhdr8kS0b"},"source":["## A Sequential Architecture to predict the hate speech status of the tweets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qlPnsir0kS0b"},"source":["The above Logistic Regression Model uses a TF-IDF matrix as input, meaning that the word ordering of the sentences is disregarded, had we not included bigrams. Next, we want to train a model that takes and *understands* sequential data inputs, as language exactly is sequential and context dependent. For instance, the word `not` is part of nltk's `stopwords.words('english')`-package. `not` can shift the meaning of a sentence completely, depending on its semantic position, but non-sequential models have no way of discerning which part of the sentence the `not` relates to. Sequential models, however, do. \n","\n","Both **Recurrent Neural Networks** (RNN) and **Long Short-Term Memory** (LSTM) have a sequential architecture, making them rather fitting for modeling the semantic meaning of language, compared to the Logistic Regression (Hovy 2022b: 73-75). The LSTM tries to mitigate the risk of exploding/vanishing gradients that the RNN might suffer from when the input sequence is long (Raschka & Mirjalili 2019:581). Thus, LSTMs are better when it comes to longer sentences. However, tweets are characterized by being short sentences with a limit of 280 characters, and our dataset might not have the proper sentence lengths for the LSTM architecture to fully flourish, compared to the RNN and Logistic Regression. Though both models could be potential candidates for our hate speech identification task, state-of-the-art language classification no longer relies on RNN or LSTM, but on their evolutionary successor, BERT (Hovy 2022b: 81). In this section, we fine-tune two BERT models with different pre-trainings - a general purpose BERT and a BERT specialized to detect hate speech on Twitter.  "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CFnDyB3qkS0r"},"source":["### A fine-tuned BERT model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Below, we define a function to load a pre-trained BERT model, tokenize our text corpus based on the model's pre-trained embeddings, and ultimately fine-tune the BERT to predict hate speech in our Twitter data.  "]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":806,"status":"ok","timestamp":1686745007335,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"Z50orgpZQGoQ"},"outputs":[],"source":["metric_f1=load_metric(\"f1\")\n","metric_acc=load_metric(\"accuracy\")\n","\n","# performance function\n","def compute_metrics(eval_pred):\n","      outputs, labels=eval_pred\n","      predictions=np.argmax(outputs, axis=-1)\n","      f1=metric_f1.compute(predictions=predictions, references=labels)\n","      acc=metric_acc.compute(predictions=predictions, references=labels)\n","      return f1 | acc\n","\n","def BERT_hate_classifier(model_name):\n","  # allow model to access the GPU\n","  device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","  # Set up the tokenizer we want to use\n","  tokenizer=AutoTokenizer.from_pretrained(model_name)\n","  # Moving tokenizer to work on GPU\n","  tokenizer.to_device=device\n","  # Apply the tokenizer to each row in the dataset\n","  tokenized_train_dataset=train_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n","  tokenized_val_dataset=val_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n","  tokenized_test_dataset=test_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n","  # Specify task for pretrained model\n","  hate_classifier=AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","  # Moving model to GPU\n","  hate_classifier.to(device)\n","  # Specify training parameters\n","  training_args=TrainingArguments(output_dir=\"bert_hatespeech\",\n","                                    evaluation_strategy=\"steps\",\n","                                    num_train_epochs=5,\n","                                    per_device_train_batch_size=16,\n","                                    logging_steps=500,\n","                                    eval_steps=500)\n","\n","  trainer=Trainer(model=hate_classifier,\n","                    args=training_args,\n","                    compute_metrics=compute_metrics,\n","                    train_dataset=tokenized_train_dataset,\n","                    eval_dataset=tokenized_val_dataset,\n","                    tokenizer=tokenizer)\n","  # fine-tune model\n","  trainer.train()\n","\n","  return trainer, tokenized_test_dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4ORn4xWWTKsW"},"source":["We train a BERT model, that is not specialized in twitter hate speech, but a more general purpose BERT trained on Wikipedia and the BookCorpus (Devlin et al. 2017)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvrgZBYdfuUn"},"outputs":[],"source":["trainer, tokenized_test_dataset=  BERT_hate_classifier(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1686743696419,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"Q6hHC1beSXhQ","outputId":"f247f22d-1384-43bf-bc3b-b15bc7d3278e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of fine-tuned BERT model: 0.77\n","F1 of fine-tuned BERT model: 0.74\n"]}],"source":["eval_on_val_data=trainer.evaluate()\n","\n","print(f\"Accuracy of fine-tuned BERT model: {eval_on_val_data['eval_accuracy']:.2f}\")\n","print(f\"F1 of fine-tuned BERT model: {eval_on_val_data['eval_f1']:.2f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YT0qBmJQwBd0"},"source":["Next, to see how much of a difference the pre-trained BERT model makes, we also train a BERT that is developed by the authors behind the TweetEval Hate Speech dataset, pre-trained to detect hate speech on twitter (Basile et al. 2019)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjJp97FSv5rS"},"outputs":[],"source":["trainer_tweeteval, tokenized_test_dataset_tweeteval= BERT_hate_classifier(\"cardiffnlp/twitter-roberta-base-hate-latest\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1686743706931,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"GUd7hTQtv73u","outputId":"dd94650e-ee1e-4fd2-cd99-8f4a2ddc911c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of fine-tuned BERT model: 0.81\n","F1 of fine-tuned BERT model: 0.79\n"]}],"source":["eval_on_val_data=trainer_tweeteval.evaluate()\n","\n","print(f\"Accuracy of fine-tuned BERT model: {eval_on_val_data['eval_accuracy']:.2f}\")\n","print(f\"F1 of fine-tuned BERT model: {eval_on_val_data['eval_f1']:.2f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cG4sF1CLkS0w"},"source":["## For each of the models you ran in question 1. and 2., briefly discuss (two–four sentences) in what ways the model is a good choice for the current task and data set, plus any downsides the model might have for this application.\n","\n","**Logistic Regression**: Since Logistic Regression uses a TF-IDF vectorization to convert words and sentences into numbers, it disregards the order with which the words appear in the sentences. That word ordering does not matter to the meaning of a sentence is a somewhat crude assumption, though with regard to this particular task of detecting hate speech, simply identifying the presence or prevalence of a \"hateful word\" could suffice. This model is also less computationally intensive to train than the rest. \n","\n","**BERT**: As oppose to the Logistic Regression Model, BERT allow for sequential data input, which is fitting for language classification. We used two pre-trained BERT-models; One that is trained on twitter data with the objective to detect hate speech, and one that is trained for more general language processing. Both BERT models had similar or better performances on during training than the Logistic Regression, with regard to `accuracy` and `F1`. The BERT model specialized on twitter hate speech performed slightly better than the general purpose BERT. Such pre-trained, specialized model is very convenient, but it also gives the model a natural head-start compared to other models. Being able to utilize the relatively better performance of BERT thus depends on the existence of a pre-trained model and also what that model is trained for. Alternatively, we would have had to pre-train the BERT-model from scratch, which would have been a much more extensive task."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NzkqwuB1kS0x"},"source":["## Which is the best-performing model in terms of F1, and should we prefer F1 over accuracy as an indicator of model performance here?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kQ_9Tj87E6kn"},"source":["We calculate the `accuracy` and `F1` score for each model on the `test data` and print the results."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Logistic Regression</th>\n","      <td>0.49</td>\n","      <td>0.61</td>\n","    </tr>\n","    <tr>\n","      <th>BERT TweetEval</th>\n","      <td>0.57</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>BERT General</th>\n","      <td>0.53</td>\n","      <td>0.64</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     Accuracy    F1\n","Logistic Regression      0.49  0.61\n","BERT TweetEval           0.57  0.65\n","BERT General             0.53  0.64"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["test_pred_lr=lr_grid.predict(test_features_lr) # LR\n","\n","output, true_labels, eval=trainer.predict(tokenized_test_dataset) # BERT General\n","acc_bert, f1_bert=eval[\"test_accuracy\"], eval[\"test_f1\"]\n","\n","output, true_labels, eval_tweeteval=trainer_tweeteval.predict(tokenized_test_dataset_tweeteval) # TweetEval BERT\n","acc_bert_tweeteval, f1_bert_tweeteval=eval_tweeteval[\"test_accuracy\"], eval_tweeteval[\"test_f1\"]\n","\n","pd.DataFrame([[accuracy_score(test_pred_lr, test_labels),\n","               f1_score(test_pred_lr, test_labels)],\n","              [acc_bert_tweeteval, f1_bert_tweeteval],\n","              [acc_bert, f1_bert]],\n","             index=[\"Logistic Regression\", \"BERT TweetEval\", \"BERT General\"],\n","             columns=[\"Accuracy\", \"F1\"]).round(2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["First of all, we notice that all models perform significantly worse on the test data, compared to their performance on the validation data. This could obviously be a modeling issue, but it could also encourage a closer look at the test datasets, to see if it comes down to an issue of noise or messy data. The results, however, are close to the one's measured by the authors of the `TweetEval Hate Speech` dataset (Basile et al. 2019). Also, whether various forms of preprocessing could have increased model performance would have to be explored if one were to further fine-tune the models for production or publication.\n","\n","Deciding how to weigh the importance of either the `accuracy` or `F1` depends on the task and what one wants to achieve - or not to achieve - with one's classification model. Accuracy has straightforward interpretability and is a good metric when the outcome classes are reasonably balanced, as is the case in our dataset. The `F1-score`, on the other hand, is less intuitively interpretable, but is a better pick for imbalanced classes. `F1` takes the `False Positives` and `False Negatives` into account, assessing both error types relative to the number of `True Positives`, rather than the `True Negatives` (Hovy 2022b: 25f). This also means that *F1* changes, depending on which class we \"focus on\" - that is, which class we assign to be a `True Positive`. In our case, we focus on hate rather than non-hate. So, is that diserable? Since our ultimate goal is hate speech detection, *F1* could be a more adequate measure, in spite of the balanced classes. Adhering to best practice, we present both metrics to provide a comprehensive evaluation of our model´s performance, and, either way, the BERT pre-trained on twitter hate speech has the better performance on both metrics, with an accuracy of 0.57 and F1 of 0.65. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"afI5KdCRkS00"},"source":["## Based on the paper by Basile et al. (2019), What further info might you have liked to have about the data selection process and/or the annotation process for this data set? Why?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_REAg-dn8bSU"},"source":["In the **data selection process**, the term `identified hater` is not further defined (Basile et al. 2019: 55). Thus, the exact sampling strategy is unclear which makes replication difficult.\n","\n","In the **annotation process**, the authors try to counter the potential inconsistency of the individual annotator by using majority voting on each tweet among three annotators (crowds). The tweet is also annotated by two experts, whereafter the final label is a majority voting between crowd, expert 1, and expert 2. The exact qualifications of the experts remains unclear in the article, aside from language capability and general field knowledge. The article provides a measure of the average confidence (AC) between the annotations, but it is unclear if this measure only relates to the crowd. It would have been informative to explicitly have both the AC between the crowd and between the crowd and the experts. Though the aggregate AC measure is informative, it could have been interesting to include a measure of agreement/reliability for each tweet in the dataset, as to get an indication of which tweets have been unanimously annotated and which one's have been more ambiguous annotated.\n","\n","Finally, we would have liked if the authors were more explicit about the constitution of their crowdsourcing provider and annotators, especially since crowdsourcing of annotative labor has a history of irresponsible working conditions. This also raise a discursive consideration, as it could also be interesting to know more about the social characteristics of the annotators and experts. Are they all from the same country or do they all identify as the same gender? There might be discursive reminiscents of these social dimensions latently embedded in the annotation of what is hate speech and what is not in the data. \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Part 4: Word embeddings"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The Word2vec algorithm is a widely used embedding implementation introduced by Le and Milov (2014) (Grimmer et al. 2022: 86; Hovy 2022b: 19). The assumption behind word2vec is that words that appear in similar contexts have similar meaning. Based on this assumption, the algorithm creates an n-dimensional vector space, where the meaning of each word is reflected by its relative position to other words. In Section 1, we saw how OHR utilized this to identify the emotiveness of non-labelled words based on their relative proximity to either emotive or neutral words in an embedding space. In this section, we will train a word2vec embedding model on the United Nations General Debate Corpus, and explore the strengths and weaknesses of word embedding for semantic analysis. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train your own word2vec vectors on the dataset. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We load, tokenize, and lowercase the data."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def load_and_process_data(filename):\n","    # to hold all sentences in the corpus\n","    corpus=[]\n","    \n","    # Open the file \n","    with open(filename, \"r\", encoding=\"utf-8\") as f:\n","        for line in f: # iterate over lines\n","            # is line empty\n","            if line.strip() != '':\n","                # Tokenize and lowercase \n","                encoded_text=[word.lower() for word in word_tokenize(line)]\n","                # Add tokens to the corpus\n","                corpus.append(encoded_text)\n","    \n","    return corpus\n","\n","# load and process the datab\n","path=\"Data\"\n","speeches=load_and_process_data(path+\"/allspeeches_77_2022.txt\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, we train our word2vec model."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def vec_function(seed):\n","    speech2vec=gensim.models.Word2Vec(\n","        speeches,         # the corpus object we've loaded\n","        vector_size=200,  # the dimensionality of the target vectors\n","        window=5,         # window ngram size\n","        min_count=4,      # ignoring low-frequency words\n","        epochs=3,         # how many training passes to have\n","        sg=1,           # 1 for skip-gram model\n","        seed=seed)        # seed for replication\n","    return(speech2vec)\n","\n","vec_seed13=vec_function(seed=13)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are two architectures which can be implemented in the Word2Vec; Skip-Gram(SG) or Continous Bag of Words (CBOW). In general SG outperforms CBOW, so we will use that approach for this exercise (Ogundepo 2021). Further, we choose the default of negative sampling for updating the weights in the neural network during training, to minimize the computational intensity. \n","\n","Aside from the parameters provided in the assignment description, we set a seed for replication, select the skip-gram model and negative samping, and choose to ignore low frequency words. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Print out the ten words that are most similar to: “climate”, “pandemic”, “terrorism” and “future” (or choose your own four words of interest). Briefly discuss anything you find noteworthy about the associations that the model has picked up."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We use the `most_similar` method, which measures the distance between words in the vector space using cosine-similarity,  "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>gender</th>\n","      <th>elizabeth</th>\n","      <th>russia</th>\n","      <th>ukraine</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>protection</td>\n","      <td>king</td>\n","      <td>ukraine</td>\n","      <td>russia</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>empowerment</td>\n","      <td>majesty</td>\n","      <td>illegal</td>\n","      <td>military</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>women</td>\n","      <td>excellency</td>\n","      <td>military</td>\n","      <td>aggression</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>services</td>\n","      <td>abdulla</td>\n","      <td>aggression</td>\n","      <td>illegal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>girls</td>\n","      <td>shahid</td>\n","      <td>russian</td>\n","      <td>russian</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>equality</td>\n","      <td>abdullah</td>\n","      <td>invasion</td>\n","      <td>forces</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>employment</td>\n","      <td>charles</td>\n","      <td>federation</td>\n","      <td>armed</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>promotion</td>\n","      <td>csaba</td>\n","      <td>forces</td>\n","      <td>crimes</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>public</td>\n","      <td>predecessor</td>\n","      <td>iran</td>\n","      <td>war</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>participation</td>\n","      <td>maldives</td>\n","      <td>war</td>\n","      <td>federation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          gender    elizabeth      russia     ukraine\n","0     protection         king     ukraine      russia\n","1    empowerment      majesty     illegal    military\n","2          women   excellency    military  aggression\n","3       services      abdulla  aggression     illegal\n","4          girls       shahid     russian     russian\n","5       equality     abdullah    invasion      forces\n","6     employment      charles  federation       armed\n","7      promotion        csaba      forces      crimes\n","8         public  predecessor        iran         war\n","9  participation     maldives         war  federation"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["word_of_interest=[\"gender\", \"elizabeth\", \"russia\", \"ukraine\"] \n","\n","#  func to find top ten most similar words\n","def find_top_words(word_of_interest, model):\n","    df=pd.DataFrame() \n","    for word in word_of_interest:\n","        # Using the most_similar function to find the 10 words\n","        similar_words=list(map(lambda x: x[0], model.wv.most_similar(word, 10)))\n","        df[word]=similar_words\n","    return(df) # Returning a dataframe\n","\n","top_words_seed13=find_top_words(word_of_interest=word_of_interest, model= vec_seed13)\n","top_words_seed13"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Starting with `gender`, we see that *empowerment*, *protection*, and *women* are the most similar words to gender. This would indicate that these words often appear in the similar contexts. It is interesting that *men* is not one of the most similar word to gender, while *women* is. This could indicate that debates on gender in the UN is mainly concerning empowerment and protection of women rather than men. A rough Beauvoirian interpretation of this could be that men are considered the *default* gender, and women is the *other*.\n","\n","Looking at `elisabeth`, it makes sense that *king*, *charles*, and *iii* are closely positioned in the vector space since they probably refer to the Queens death in 2022 and her son and successor King Charles III. Oddly, the words *abdulla(h)* and *shahid* also appear as similar, which probably refers to Abdullah Shahid, a key politician within the UN, who is often addressed as \"his exellence\" in the text corpus. \n","\n","Both `russia` and `ukraine` respectively have each other as the most similar words. It makes sense then, that they also share many of the same words as their most similar ones, e.g. military, aggresion, and war. Since they are both each others most similar word, they are positioned rather close to each other in the vector space, therefore also sharing their proximity to all the other words. This makes it difficult to discern which words are targeting which of the countries, e.g. is the similarity between *illigal* and *ukrain* mainly because ukrain is close to russia in the vector space? And which one of the countries are commiting the *aggresion*?  "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Suppose that we would like to use these word embeddings as input in a supervised model, detecting whether the speech comes from a country in the global North or the global South. Briefly discuss the upside(s) and downside(s) of training your word embeddings locally, on the speeches themselves, versus using pre-trained embeddings."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Deciding whether to train our own word embeddings locally or use pre-trained embeddings, it is important to consider how similar or different one's text corpus of interest is to the copera of the pre-trained embeddings (Rodriguez & Spirling 2022: 6). Using pre-trained embeddings comes with the risk of inaccuracy due to lacking domain specificity compared to one's corpus of interest - say we load embeddings trained on Wikipedia to use for our UN speech analysis (Rodriguez & Spirling 2022: 6). One could also encounter that words are not within the pre-trained embedding's vocabulary, thus losing the potential information in out-of-vocabulary words (Grimmer et al. 2022: 84f). On the other, data accessibility and computing power can be a problem for locally trained embeddings. The pre-trained embeddings are often trained on hundreds of billions of tokens, while we only have 77 speeches in the UN. If the copora of the pre-trained embeddings is related to one's local corpus - for instance if both are texts on political speecehs - one can save some computation by opting for pre-trained embeddings. In general Rodriguez & Spirling find that the pre-trained embeddings often do better than the locally trained models (Rodriguez & Spirling 2022: 15-16). \n","\n","A limitation to the embedding framework is that the vector representations are fixed when training is completed, meaning that the exact context of the words are average out (Grimmer et al. 2022: 88). Thus, discerning between global North or global South could be difficult using word2vec's vector representations, as the potential contextual nuaces and differences of a particular word between North and South is aggregated across all speeches and fixed into its average contextual meaning. In newer models such as BERT, they have implemented contextual embeddings, which are better at capturing semantic nuances in the language. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## (optional) Train the model again. Are the word embeddings stable?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Not answered due to character limitation."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## (optional) Conduct an informal validation of the embeddings from your first run, by checking their ability to find the “odd one out” in three different series of four–five terms related to international relations and current events (e.g. “covid”, “pandemic”, “disease”, “vaccine”, “environment”). Briefly discuss how you might validate the embeddings more systematically, if you had more time and resources. "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["For 'war' the odd one out is: guns\n","For 'climate' the odd one out is: floods\n","For 'nations' the odd one out is: russia\n","For 'covid 19' the odd one out is: environment\n"]}],"source":["list_of_lists_of_words={\n","    \"war\": [\"bombs\", \"guns\", \"ammo\", \"tanks\" \"cake\"],\n","    \"climate\": [\"floods\", \"drought\", \"heatwave\", \"hallo\"],\n","    \"nations\": [\"china\", \"america\", \"russia\", \"hammer\"],\n","    \"covid 19\": [\"covid\", \"pandemic\", \"disease\", \"vaccine\", \"environment\"]\n","}\n","for key, value in list_of_lists_of_words.items():\n","    print(f\"For '{key}' the odd one out is: {vec_seed13.wv.doesnt_match(value)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The model does not do a particularly good job of identifying the \"odd one out\". It only gets ´environment´ right in the covid 19 list. If one had the time and ressources, a more systematic validation could be conducted using the approach presented by Rodriguez & Spirling (2021). For a given target word, they construct two seperate lists of the 10 most similar words according to either human annotators or their embedding model (Rodriguez & Spirling 2021: 11). They then ask a seperate group of humans to choose whether the embedding model or the human annotators have provided the most fitting list of similar words. By applying this approach to a subset of relevant target words, it could provide a somewhat systematic, yet rather costly validation of one's embedding model. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gvm_xNm3JGfm"},"source":["## Literature\n","\n","**Basile, V., Bosco, C., Fersini, E., Nozza, D., Patti, V., Rangel Pardo, F. M., Rosso, P., & Sanguinetti, M.** (2019). SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter. In *Proceedings of the 13th International Workshop on Semantic Evaluation* (pp. 54-63). Minneapolis, Minnesota, USA: Association for Computational Linguistics. [Link](https://www.aclweb.org/anthology/S19-2007) (DOI: [10.18653/v1/S19-2007](https://doi.org/10.18653/v1/S19-2007))\n","\n","**Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova-** 2018. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" CoRR, abs/1810.04805. Retrieved from http://arxiv.org/abs/1810.04805.\n","\n","**Raschka, Sebastian, V. Mirjalili** (2019). Python Machine Learning;Machine Learning and Deep Learning with Python Scikit-Learn and Tensorflow 2 3rd Edition. https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=2329991. Accessed June 14 2023.\n","\n","**Ogundepo, Odunayo**  (2021): \"Understanding Word2Vec\", Medium, https://medium.com/analytics-vidhya/understanding-word2vec-39fabe660705"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
