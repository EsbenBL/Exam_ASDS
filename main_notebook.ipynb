{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Social Data Science 2\n",
    "\n",
    "*By Carl, Asger, & Esben*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gensim.downloader\n",
    "import torch\n",
    "import re\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import psutil\n",
    "import platform\n",
    "from datasets import load_metric, load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Supervised text classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the training data set to fit at least one of the following models to try and predict the hate speech status of the tweets, using TF-IDF features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the `tweet_eval` dataset, split into train, validation, and test. `0 = non-hate` and `1 = hate`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (C:/Users/45242/.cache/huggingface/datasets/tweet_eval/hate/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Found cached dataset tweet_eval (C:/Users/45242/.cache/huggingface/datasets/tweet_eval/hate/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Found cached dataset tweet_eval (C:/Users/45242/.cache/huggingface/datasets/tweet_eval/hate/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    }
   ],
   "source": [
    "train_data = load_dataset(\"tweet_eval\", \"hate\", split = \"train\")\n",
    "val_data = load_dataset(\"tweet_eval\", \"hate\", split = \"validation\")\n",
    "test_data = load_dataset(\"tweet_eval\", \"hate\", split = \"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of observations within each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets in train: 9000\n",
      "Tweets in validation: 1000\n",
      "Tweets in test: 2970\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tweets in train: {train_data.shape[0]}\")\n",
    "print(f\"Tweets in validation: {val_data.shape[0]}\")\n",
    "print(f\"Tweets in test: {test_data.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text and labels into seperate lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [tweet[\"text\"] for tweet in train_data]\n",
    "train_labels = [tweet[\"label\"] for tweet in train_data] \n",
    "test_corpus = [tweet[\"text\"] for tweet in test_data]\n",
    "test_labels = [tweet[\"label\"] for tweet in test_data] \n",
    "val_corpus = [tweet[\"text\"] for tweet in val_data]\n",
    "val_labels = [tweet[\"label\"] for tweet in val_data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine class distribution in the different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribtion in train: {'non-hate': 5217, 'hate': 3783}\n",
      "Label distribtion in validataion: {'non-hate': 573, 'hate': 427}\n",
      "Label distribtion in test: {'non-hate': 1718, 'hate': 1252}\n"
     ]
    }
   ],
   "source": [
    "label_dict = {0:\"non-hate\", 1:\"hate\"}\n",
    "\n",
    "print(f\"Label distribtion in train: {dict(Counter([label_dict[label] for label in train_labels]))}\")\n",
    "print(f\"Label distribtion in validataion: {dict(Counter([label_dict[label] for label in val_labels]))}\")\n",
    "print(f\"Label distribtion in test: {dict(Counter([label_dict[label] for label in test_labels]))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing - to avoid an extensive process of providing accurate Part-of-Speech-tags to the lemmatizer, as well as the increased computational cost, we only stem the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(_str):\n",
    "    # remove twittertags + lowercase \n",
    "    _str = re.sub(r'@\\w+', \"\", _str.lower())\n",
    "    # remove numbers\n",
    "    _str = re.sub(r'\\d+', \"\", _str) \n",
    "    # remove punctuations \n",
    "    _str = _str.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"!\",\"\")))\n",
    "    # Remove extra whitespaces \n",
    "    _str = re.sub(r'\\s+', ' ', _str.strip())  \n",
    "    # tokenize text - we do not use TweetTokenize as we have remove @ either way\n",
    "    tokens = word_tokenize(_str)\n",
    "    # remove stopwords and stem\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    return ' '.join(tokens) # join words back into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_preproc = [preproc(tweet) for tweet in train_corpus]\n",
    "test_corpus_preproc = [preproc(tweet) for tweet in test_corpus]\n",
    "val_corpus_preproc = [preproc(tweet) for tweet in val_corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerations on the preprocessing**: As preprocessing is more of an assessment than a hard science, we have made some decision that we think are best suited for the task at hand, i.e. to classify hate-speech on twitter, some of these are:\n",
    "- Retaining `!` when removing punctiation, as exclamations could be useful for the model when predicting hate-speech. \n",
    "- Though capital letters could convey some meaning when predicting hate-speech, We have lower-cased all words, to make the classification more about semnatic meaning than letter-capitalization.  \n",
    "\n",
    "Ultimalty, how one preprocesses data is about how to convey relevant nuances in the data, in the most simplified way, e.g. by removing stopwords, if these do not provide any information to the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `TfidfVectorizer()` to convert the documents into a TF-IDF feature matrix. We use both `unigrams` and `bigrams` to give the model a little more context about the context of the words - i.e. neighboring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=\"word\",\n",
    "                             ngram_range = (1,2))\n",
    "                             \n",
    "train_features = vectorizer.fit_transform(train_corpus_preproc)\n",
    "# only transform() on val and test, to make the evaluation resemeble \"unseen data\" more\n",
    "val_features = vectorizer.transform(val_corpus_preproc)  \n",
    "test_features = vectorizer.transform(test_corpus_preproc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with ridge regularization (with a c parameter tuned via cross-validation on the train set)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the hyperparameter, `C`, for regularizing the model to avoid overfitting, we carry out a `GridSearchCV`, with 5 folds and spanning `np.logspace(-2, 2, 50)` values of `C`. We use ridge-regression for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.77\n",
      "Best parameters:  {'C': 32.374575428176435}\n",
      "Best estimator:  LogisticRegression(C=32.374575428176435, max_iter=1000, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"C\": np.logspace(-2, 2, 50)} # search from 10^-2 to 10^2 with 100 steps\n",
    "\n",
    "lr_grid = GridSearchCV(LogisticRegression(penalty=\"l2\", random_state=0,\n",
    "                        max_iter = 1000), \n",
    "                        param_grid = param_grid, \n",
    "                        cv=5, \n",
    "                        n_jobs=-1, \n",
    "                        scoring = \"accuracy\")\n",
    "\n",
    "lr_grid.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(lr_grid.best_score_))\n",
    "print(\"Best parameters: \", lr_grid.best_params_)\n",
    "print(\"Best estimator: \", lr_grid.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with a linear kernel (with a c parameter tuned via crossvalidation on the train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.78\n",
      "Best parameters:  {'C': 1.0985411419875581}\n",
      "Best estimator:  LinearSVC(C=1.0985411419875581, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"C\": np.logspace(0, 2, 50)} # search from 10^-10 to 10^2 with 30 steps\n",
    "\n",
    "lscv_grid = GridSearchCV(LinearSVC(penalty=\"l2\", random_state=0), \n",
    "                        param_grid = param_grid, \n",
    "                        cv=5, \n",
    "                        n_jobs=-1, \n",
    "                        scoring = \"accuracy\")\n",
    "\n",
    "lscv_grid.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(lscv_grid.best_score_))\n",
    "print(\"Best parameters: \", lscv_grid.best_params_)\n",
    "print(\"Best estimator: \", lscv_grid.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes does not take the `TfidfVectorizer()` as input, but rather `CountVectorizer()`, i.e. simply the word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes cross-validation accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "counterizer = CountVectorizer()\n",
    "train_counts = counterizer.fit_transform(train_corpus)\n",
    "val_counts = counterizer.transform(val_corpus)\n",
    "test_counts = counterizer.transform(test_corpus)\n",
    "\n",
    "nb_score_acc = cross_val_score(MultinomialNB(), \n",
    "                           train_counts.toarray(), \n",
    "                           train_labels, \n",
    "                           cv=5, \n",
    "                           scoring=\"accuracy\", \n",
    "                           verbose=2)\n",
    "\n",
    "print(f\"Naive Bayes cross-validation accuracy:  {np.mean(nb_score_acc):.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also fit at least one of the following models to try and predict the hate speech status of the tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the following models take and *\"understands\"* sequential data inputs, as opposed to the previous models, who use a bag-of-words or Tf-IDF matrix as inputs, where the order the the words in the sentence are neglected. Thus, we have to preprocess the input data differently. For instance, the word `not` is part of nltk's `stopwords.words('english')`-package. The word `not` can shift the meaning of a sentence completly, depending on it's position. Non-sequential datastructures and models have no way of decerning which part of the sentece the `not` relates to - but sequential models do. Thus, we will commit a much less extensive data cleaning process in the preprocessing phase, and only lower-case. `Stemming` and `Lemmatizing` would also reduce the semantic meaning of the words, so this will not commit either of these steps as well. BLABLA, måske en henvisning til noget.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"not\" in stopwords.words('english')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to represent each token through an embedding, rather than as a Tf-Idf-score or word-count, we use the pretrained embeddings from `glove-twitter-200` to embed the features in our vocabulary into a 200-dimensional vector space. One of the main advantages of this kind of text `vectorization` - e.g. compared to one-hot encoding - is to reduced the dimensionality of the feature space and decrease the effect of the curse of dimensionality (Raschka & Mirjalili 2019:590). With TF-Idf, the feature matrix had the dimension `vocabulary_length x n_documents` - 88.987 x 9.000 in the above models, where both unigrams and bigrams were included - and was a sparse matrix mostly containing zeros. This is not the case with embeddings, where we reduce the dimenensions of the feature matrix to `vocabulary_length x embedding_dimension` (24.020 x 200).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained embeddings (these can be used as the embedding argument in create_embedding_matrix)\n",
    "glove = gensim.downloader.load('glove-twitter-200')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to establish our total vocabulary. We lowercase all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the full list of vocabulary in the tweet_eval data\n",
    "total_vocabulary = set()\n",
    "for tweet in train_corpus + val_corpus+test_corpus:\n",
    "    tokens = word_tokenize(tweet)\n",
    "    for t in tokens:\n",
    "        total_vocabulary.add(t.lower()) # lower.case\n",
    "total_vocabulary = sorted(list(total_vocabulary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include a padding-token, \"\", as the first entry, which will be the first row in the embedding matrix. We are going to ensure that the input sequences - i.e. tokenized sentences - have the same length, which we will do by continously appending empty string token to the front of the list of tokens in sentences that are shorter than the longest sentece. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending an empty padding token at the beginning of the vocabulary\n",
    "total_vocabulary = [\"\"]+total_vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a embedding matrix with the dimension `vocabulary_length x embedding_dimension`. These 200-dimensional vector will serve as representations of the features and be the input to the RNN and LSTM. Tokens that are not already in the pretrained embeddings will have all their values set to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24019/24019 [00:00<00:00, 207147.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.1390507910075 % of tokens are out of vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(tokens, embedding):\n",
    "    \"\"\"creates an embedding matrix from pre-trained embeddings for a new vocabulary. It also adds an extra vector\n",
    "    of zeroes in row 0 to embed the padding token, and initializes missing tokens as vectors of 0s\"\"\"\n",
    "    oov = set()\n",
    "    size = embedding.vector_size\n",
    "    # note the extra zero vector that will used for padding\n",
    "    embedding_matrix=np.zeros((len(tokens),size))\n",
    "    c = 0\n",
    "    for i in tqdm(range(1,len(tokens))):\n",
    "        try:\n",
    "            embedding_matrix[i]=embedding[tokens[i]]\n",
    "        except KeyError: #to catch the words missing in the embeddings\n",
    "            try:\n",
    "                embedding_matrix[i]=embedding[tokens[i].lower()]\n",
    "            except KeyError:\n",
    "                #if the token does not have an embedding, we initialize it as a vector of 0s\n",
    "                embedding_matrix[i] = np.zeros(size)\n",
    "                #we keep track of the out of vocabulary tokens\n",
    "                oov.add(tokens[i])\n",
    "                c +=1\n",
    "    print(f'{c/len(tokens)*100} % of tokens are out of vocabulary')\n",
    "    return embedding_matrix, oov\n",
    "\n",
    "#get the embedding matrix and out of vocabulary words for our tweet_eval vocabulary\n",
    "embedding_matrix, oov = create_embedding_matrix(total_vocabulary, glove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text sentences into a list of indicies, each of which corresponding to the words' indices in the total vocabulary and in the embedding matrix. Also, we use padding on the feature vectore to ensure that all vector respresentations of the input sentences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(text, total_vocabulary):\n",
    "    \"\"\"turns the input text (one tweet) into a vector of indices in total_vocabulary that corresponds to the tokenized words in the input text\"\"\"\n",
    "    encoded_text = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for t in tokens:\n",
    "        index = total_vocabulary.index(t.lower())\n",
    "        encoded_text.append(index)\n",
    "    return encoded_text\n",
    "\n",
    "def add_padding(vector, max_length, padding_index):\n",
    "    \"\"\"adds copies of the padding token to make the input vector the max_length size, so that all inputs are the same length\n",
    "    (the length of tweet with most words)\"\"\"\n",
    "    if len(vector) < max_length:\n",
    "        vector = [padding_index for _ in range(max_length-len(vector))] + vector\n",
    "    return vector\n",
    "\n",
    "# vectorize sentences to indices\n",
    "train_features = [text_to_indices(text, total_vocabulary) for text in train_corpus]\n",
    "val_features = [text_to_indices(text, total_vocabulary) for text in val_corpus]\n",
    "test_features = [text_to_indices(text, total_vocabulary) for text in test_corpus]\n",
    "\n",
    "# calc length of longest tweet\n",
    "longest_tweet = max(train_features+val_features, key=len)\n",
    "max_length = len(longest_tweet)\n",
    "padding_index = 0 #position 0 is where we had put the padding token in our vocabulary and embedding matrix\n",
    "\n",
    "# padding the feature vectors\n",
    "train_features = [add_padding(vector, max_length, padding_index) for vector in train_features]\n",
    "val_features = [add_padding(vector, max_length, padding_index) for vector in val_features]\n",
    "test_features = [text_to_indices(text, total_vocabulary) for text in test_corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert all splits of the data into a PyTorch DataSet as well as a DataLoader, to perform `mini batch gradient descent` when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetEvalTrain(torch.utils.data.Dataset):\n",
    "    # defining the sources of the data\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = torch.LongTensor(features)\n",
    "        self.y = torch.from_numpy(np.array(labels))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index].unsqueeze(0)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "data_train = TweetEvalTrain(train_features, train_labels)\n",
    "data_val = TweetEvalTrain(val_features, val_labels)\n",
    "data_test = TweetEvalTrain(val_features, val_labels) \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=64)\n",
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we the data is ready to be fed into the RNN and LSTM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many-to-one RNN with pre-trained word embeddings as the inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define both a custom RNN/LSTM class, a training_loop function and an evaluation function. Note that we do not need to explicitly define a softmax-activation for the outputs to be passed through, as our loss function, `CrossEntropyLoss`, automatically applies a softmax transformation before calculating the loss. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our RNN and LSTM classes, where the number of hidden layers can be change using the `num_layers` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the embedding step and RNN model\n",
    "\n",
    "class RNN_LSTM(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix, num_layers=1, type_=\"RNN\"):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True) # the tokens corresponding to the padding_idx will not be included in the training of the model \n",
    "        emb_dim = embedding_matrix.shape[1] # = 200 in this case --> It is our input_size, and it represents just one word, but through 200 features! That's why there are also 200 weights between the input layer and each node in the hidden layer.\n",
    "        self.num_layers = num_layers\n",
    "        self.type = type_ \n",
    "\n",
    "        #remember the batch_first=True argument\n",
    "        if self.type == \"RNN\":\n",
    "            self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=self.num_layers, batch_first=True)\n",
    "        elif self.type == \"LSTM\":\n",
    "            self.rnn = torch.nn.LSTM(input_size=emb_dim, hidden_size=rnn_size, bidirectional=False, num_layers=self.num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise LookupError(\"Only RNN and LSTM are supported.\")\n",
    "\n",
    "        #define the output layer (no softmax needed here; we will apply softmax as part of the loss calculation)\n",
    "        #applies a linear transformation to the RNN\n",
    "        #final layer state and outputs scores for the n classes\n",
    "        self.linear_outputs = torch.nn.Linear(rnn_size, n_classes)                      \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "\n",
    "        # The RNN returns two tensors: one representing the hidden states at all positions,\n",
    "        # and another representing only the final hidden states.\n",
    "        # In this many-to-one model, we only need the final hidden states,\n",
    "        # i.e the prediction at the end of the sequence, where all prio information is somehow present\n",
    "        \n",
    "        # run the final state through the output layer\n",
    "        if self.type == \"RNN\" and self.num_layers > 1:\n",
    "            all_states, final_state = self.rnn(encoded_inputs)\n",
    "            outputs = self.linear_outputs(final_state[-1,:,:]) # only activation from the last layer in the stack of layers\n",
    "\n",
    "        elif self.type == \"RNN\" and self.num_layers == 1:\n",
    "            all_states, final_state = self.rnn(encoded_inputs)\n",
    "            outputs = self.linear_outputs(final_state.squeeze())\n",
    "\n",
    "        elif self.type == \"LSTM\" and self.num_layers > 1:\n",
    "            all_states, (final_state, c_n) = self.rnn(encoded_inputs)\n",
    "            outputs = self.linear_outputs(final_state[-1,:,:]) # only activation from the last layer in the stack of layers\n",
    "            \n",
    "        elif self.type == \"LSTM\" and self.num_layers == 1:\n",
    "            # LSTM's output is different and needs to be treated differently, see documentation for details\n",
    "            all_states, (final_state, c_n) = self.rnn(encoded_inputs)\n",
    "            outputs = self.linear_outputs(final_state.squeeze()) \n",
    "\n",
    "        return outputs\n",
    "\n",
    "# training loop\n",
    "def training_loop(model, num_epochs):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad() # zero the gradients that are stored from the previous optimization step\n",
    "            outputs = model.forward(inputs).squeeze() # compute the outputs of the model\n",
    "            targets = targets.squeeze().long() #dependending on your torch version you might have to use targets = targets.squeeze().long()\n",
    "            loss = loss_function(outputs, targets) #loss function (Adam) compares model output and the true labels\n",
    "            loss.backward() # Backpropagation -> get derivative of loss function \n",
    "            optimizer.step() # optimize based on derivative\n",
    "            losses.append(loss.item()) #add this batch's loss to the losses for this epoch\n",
    "        print(f'Epoch {epoch+1}: loss {np.mean(losses)}')\n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad(): # for evaluation we don't backpropagate and update weights anymore\n",
    "        for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "            outputs = torch.softmax(model(inputs), 1 ) # apply softmax to get probabilities/logits\n",
    "            # getting the indices of the logit with the highest value, which corresponds to the predicted class (as labels 0, 1, 2)\n",
    "            vals, indices = torch.max(outputs, 1)\n",
    "            # accumulating the predictions\n",
    "            predictions += indices.tolist()\n",
    "            # accumulating the true labels\n",
    "            labels += targets.tolist()\n",
    "    \n",
    "    acc = accuracy_score(predictions, labels)\n",
    "    f1 = f1_score(predictions, labels)\n",
    "    print(f'Model accuracy: {acc:.2f}')\n",
    "    print(f'Model F1: {f1:.2f}')\n",
    "    return acc, f1, predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train an RNN with a single hidden layer with 100 nodes in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.6092735932651141\n",
      "Epoch 2: loss 0.5558912851286273\n",
      "Epoch 3: loss 0.5278104945277491\n",
      "Epoch 4: loss 0.5224494695240725\n",
      "Epoch 5: loss 0.49797738908875916\n",
      "Model accuracy: 0.677\n",
      "Model F1: 0.6515641855447681\n"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 1, embedding_matrix=embedding_matrix)\n",
    "\n",
    "myRNN = training_loop(myRNN, num_epochs = 5)\n",
    "acc_single_layer_rnn, f1_single_layer_rnn, preds = evaluate(myRNN, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Deep RNN with three hidden layers and 100 nodes in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.6023058775046193\n",
      "Epoch 2: loss 0.5521705442286552\n",
      "Epoch 3: loss 0.5316433209054013\n",
      "Epoch 4: loss 0.511918028407063\n",
      "Epoch 5: loss 0.522241399431905\n",
      "Model accuracy: 0.7\n",
      "Model F1: 0.6629213483146067\n"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "my_deep_RNN = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "my_deep_RNN = training_loop(my_deep_RNN, num_epochs = 5)\n",
    "acc_deep_rnn, f1_deep_rnn, preds = evaluate(my_deep_RNN, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single hidden layer LSTM with 100 nodes in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.5865210112950481\n",
      "Epoch 2: loss 0.5176968325114419\n",
      "Epoch 3: loss 0.4854050388572909\n",
      "Model accuracy: 0.724\n",
      "Model F1: 0.6674698795180724\n"
     ]
    }
   ],
   "source": [
    "myLSTM = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 1, embedding_matrix=embedding_matrix, type_=\"LSTM\")\n",
    "\n",
    "myLSTM = training_loop(myLSTM, num_epochs = 3)\n",
    "acc_single_layer_LSTM, f1_single_layer_LSTM, preds = evaluate(myLSTM, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LSTM with 3 hidden layers of 100 nodes in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.5959317969514969\n",
      "Epoch 2: loss 0.5246773632705635\n",
      "Epoch 3: loss 0.4867223693546674\n",
      "Model accuracy: 0.723\n",
      "Model F1: 0.6775320139697322\n"
     ]
    }
   ],
   "source": [
    "my_deep_LSTM = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 3, embedding_matrix=embedding_matrix, type_=\"LSTM\")\n",
    "\n",
    "my_deep_LSTM = training_loop(my_deep_LSTM, num_epochs = 3)\n",
    "acc_deep_LSTM, f1_deep_LSTM, preds = evaluate(my_deep_LSTM, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fine-tuned BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is CPU Intel64 Family 6 Model 158 Stepping 10, GenuineIntel. GPU is not available rn\n",
      "Total CPU memory: 16.89 GB\n"
     ]
    }
   ],
   "source": [
    "# GPU housekeeping code: you do not need to modify anything, simply\n",
    "# read through it to understand what is going on, and run as is\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# a helper function to format byte counts into KB, MB and so on\n",
    "def bytes_format(b):\n",
    "    if b < 1000:\n",
    "              return f'{b} B'\n",
    "    elif b < 1000000:\n",
    "        return f'{round(float(b/1000),2)} KB'\n",
    "    elif b < 1000000000:\n",
    "        return f'{round(float(b/1000000),2)} MB'\n",
    "    else:\n",
    "        return f'{round(float(b/1000000000),2)} GB'\n",
    "\n",
    "# a helper function to check the amount of available memory\n",
    "def memory_report():\n",
    "  if device!='cpu':\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name()}\")\n",
    "    #print(torch.cuda.memory_summary())\n",
    "    total = torch.cuda.get_device_properties(0).total_memory\n",
    "    reserved = torch.cuda.memory_reserved(0)\n",
    "    allocated = torch.cuda.memory_allocated(0)\n",
    "  #  free = reserved-allocated  # free inside memory_reserved\n",
    "    print(f\"Total cuda memory: {bytes_format(total)}, reserved: {bytes_format(reserved)}, allocated: {bytes_format(allocated)}\")\n",
    "  else:\n",
    "    # Print total memory available on CPU\n",
    "    print(f'Device is CPU {platform.processor()}. GPU is not available rn')\n",
    "    total_memory = psutil.virtual_memory().total\n",
    "    print(f\"Total CPU memory: {bytes_format(total_memory)}\")\n",
    "\n",
    "memory_report()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a model that is trained to detect hate speech on twitter specialized in hate speech against women and immigrant - https://huggingface.co/cardiffnlp/twitter-roberta-base-hate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-hate\"\n",
    "\n",
    "# Set up the tokenizer we want to use\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Moving tokenizer to work on GPU \n",
    "tokenizer.to_device = device\n",
    "\n",
    "# Apply the tokenizer to each row in the dataset\n",
    "tokenized_train_dataset = train_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n",
    "tokenized_val_dataset = val_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n",
    "tokenized_test_dataset = test_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initializing the pre-trained model using the AutoModelForSequenceClassification module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## default num_labels er vidst 2. \n",
    "hate_classifier = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels = 2)\n",
    "\n",
    "# Moving model to GPU\n",
    "hate_classifier.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"bert_hatespeech\",\n",
    "                                  evaluation_strategy = \"steps\", \n",
    "                                  num_train_epochs=5,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  logging_steps=500,\n",
    "                                  eval_steps=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to calculate both `f1` and `accuracy` metrics for the predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_f1 = load_metric(\"f1\")#use the load_metric method from the datasets library to load f1 from sklearn\n",
    "metric_acc = load_metric(\"accuracy\")\n",
    "\n",
    "# Defining a function that computes it given a tuple of outputs and labels\n",
    "def compute_metrics(eval_pred):\n",
    "    outputs, labels = eval_pred\n",
    "    predictions = np.argmax(outputs, axis=-1)\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels)\n",
    "    acc = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    return f1 | acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model = hate_classifier,\n",
    "                  args = training_args,\n",
    "                  compute_metrics = compute_metrics,\n",
    "                  train_dataset = tokenized_train_dataset,\n",
    "                  eval_dataset = tokenized_val_dataset,\n",
    "                  tokenizer = tokenizer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each of the models you ran in question 1. and 2., briefly discuss (two–four sentences) in what ways the model is a good choice for the current task and data set, plus any downsides the model might have for this application.\n",
    "\n",
    "- **Logistric Regression**: Since Logistic Regression uses a Tf-IDF vectorization to convert words and sentences into numbers, it disregards the order with which the words appear in the sentences. That word ordering does not matter to the meaning of a sentence is somewhat of a crude assumption, though with regard to this particular task of detecting hate speech, simply identifying the presence or prevalence of \"hateful word\" could suffice.  \n",
    "\n",
    "- **RNN**: The RNN architecture is a good fit for predicting on twitter data, as it allows for sequential output... Skriv færdig senere, og slå eventuelt RNN og LSTM sammen. \n",
    "\n",
    "- **LSTM**: This model architectur tries to mitigate the risk of exploding/vanishing gradients. Thus, this model is better when it comes to longer sentences, as the core concept of an LSTM is to navigate what to remember from the previous parts of the sequence, and what to forget. However, tweets are characterised by being short sentences with a limit of 280 characters, and our dataset might not have the proper sentence lengths for the LSTM architecture to fully flurish, compared to the RNN and Logistic Regression. \n",
    "\n",
    "- **RNN and LSTM**: The fact that both these models are sequential, makes them rather fitting for model the semantic meaning of language, compared to the Td-Idf vectorization of the Logistic Regression. However, using pretrained embeddings for the text vectorization as we did with RNN and LSTM, the performances of our models are influenced by the  corpus used to construct the n-dimensional embedding vector space. For instance in relation to inherent biases in the embeddings stemming from the data on which it was trained, or if the vocabulary of the embeddings is small, wherefor many of the tokens from our own dataset might not be embed-able as they are not part of the embeddings vocabulary. We saw that around 30% of the tokens were not in the pretrained vocabulary, so almost a third of our vocabulary will not be used to train the model or when predicting. \n",
    "\n",
    "- **BERT**: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Present the performance of your models on the test set in terms of both F1 and accuracy. Which is the best-performing model in terms of F1? In your opinion, should we prefer F1 over accuracy as an indicator of model performance here, and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 0.49\n",
      "F1-score for Logistic Regression: 0.61\n"
     ]
    }
   ],
   "source": [
    "test_pred_lr = lr_grid.predict(test_features)\n",
    "\n",
    "print(f\"Accuracy of Logistic Regression: {accuracy_score(test_pred_lr, test_labels):.2f}\")\n",
    "print(f\"F1-score for Logistic Regression: {f1_score(test_pred_lr, test_labels):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of linear SVM: 0.49\n",
      "F1-score for linear SVM: 0.61\n"
     ]
    }
   ],
   "source": [
    "test_pred_lscv = lscv_grid.predict(test_features)\n",
    "\n",
    "print(f\"Accuracy of linear SVM: {accuracy_score(test_pred_lscv, test_labels):.2f}\")\n",
    "print(f\"F1-score for linear SVM: {f1_score(test_pred_lscv, test_labels):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes: 0.48\n",
      "F1-score for Naive Bayes: 0.61\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_counts.toarray(), train_labels)\n",
    "np_preds = nb.predict(test_counts.toarray())\n",
    "\n",
    "print(f\"Accuracy of Naive Bayes: {accuracy_score(np_preds, test_labels):.2f}\")\n",
    "print(f\"F1-score for Naive Bayes: {f1_score(np_preds, test_labels):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Layer RNN: \n",
      "Model accuracy: 0.69\n",
      "Model F1: 0.67\n",
      "\n",
      "Deep RNN: \n",
      "Model accuracy: 0.70\n",
      "Model F1: 0.66\n",
      "\n",
      "Single Layer LSTM: \n",
      "Model accuracy: 0.72\n",
      "Model F1: 0.67\n",
      "\n",
      "Deep LSTM: \n",
      "Model accuracy: 0.72\n",
      "Model F1: 0.68\n"
     ]
    }
   ],
   "source": [
    "print(\"Single Layer RNN: \")\n",
    "acc, f1, preds = evaluate(myRNN, test_loader)\n",
    "print(\"\\nDeep RNN: \")\n",
    "acc, f1, preds = evaluate(my_deep_RNN, test_loader)\n",
    "print(\"\\nSingle Layer LSTM: \")\n",
    "acc, f1, preds = evaluate(myLSTM, test_loader)\n",
    "print(\"\\nDeep LSTM: \")\n",
    "acc, f1, preds = evaluate(my_deep_LSTM, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding how to weigh the importance of either the `accuracy` or `f1` depends on the task and what one wants to achieve - or not to achieve - with one's classficiation model. \n",
    "\n",
    "As previously presented, the class-balance between `hate` and `non-hate` is fairly evenly distributed, and accuracy can then serve as an appropriate performance metric along with its straight forward interpretability, i.e. how many tweets did the classifier predict correctly. \n",
    "\n",
    "The `F1-score`, on the other hand, is less intuitivelty interpretable, but would likely have been a better pick, had the classes been imbalanced. The F1 score also cares about how the classes have been numerically encoded, meaning which class is a `Positive` and which is a `Negative`. In our case, we have assign hate speech to be positives, i.e having a value of one. That is because the `F1-score` depend on `False Positives` and `False Negatives` - i.e. if the model incorrectly classifies something as hate speech or if it fails to classify hate speech as such - both in relation to how many `True Positives` the model have classified. Thus, F1-score cares about which type of error the model makes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the paper by Basile et al. (2019), What further info might you have liked to have about the data selection process and/or the annotation process for this data set? Why?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam-asds-0J4d57eE-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
