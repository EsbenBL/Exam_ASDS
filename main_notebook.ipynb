{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"LUtlvdadkS0D"},"source":["# Advanced Social Data Science 2\n","\n","*By Carl, Asger, & Esben*"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1686728066849,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"bJdrKTFxksPS"},"outputs":[],"source":["# !pip install datasets==2.2.1 transformers==4.19.1"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17580,"status":"ok","timestamp":1686739356555,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"HGlKMLJVkS0J","outputId":"55f81027-e072-4168-c1c5-ec47086beb69"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import string\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, f1_score\n","import gensim.downloader\n","import torch\n","import re\n","from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n","import psutil\n","import platform\n","from datasets import load_metric, load_dataset\n","import numpy as np\n","from tqdm import tqdm\n","from google.colab import drive\n","import nltk\n","import pandas as pd\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aLNZQNEBkS0L"},"source":["# Part 3 Supervised text classification\n","\n","In this section, we will train a range of models to predict whether a sentence could be classified as hate speech or not. We will use a ridge regularized Logistic Regression, a Recurrent Neural Network (RNN), a deep RNN, a Long Short-Term Memomry model (LSTM), and two BERT models with different pre-trainings - a general purpose BERT and a BERT specialized to detect hate speech on Twitter.  "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zAkxIs8JkS0M"},"source":["## Use the training data set to fit at least one of the following models to try and predict the hate speech status of the tweets, using TF-IDF features."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p5AkzQcGkS0N"},"source":["First, we load the `tweet_eval` dataset and split the data into train, validation, and test. In the data `0` signifies non-hate and `1` indicates hate."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1352,"status":"ok","timestamp":1686743788988,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"bZRqtUdJkS0O","outputId":"feb0db19-b532-486b-bb02-bf0da9b08bd8"},"outputs":[],"source":["train_data = load_dataset(\"tweet_eval\", \"hate\", split = \"train\")\n","val_data = load_dataset(\"tweet_eval\", \"hate\", split = \"validation\")\n","test_data = load_dataset(\"tweet_eval\", \"hate\", split = \"test\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CWxqCQaOkS0Q"},"source":["Check the number of observations within each split."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686743788988,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"VejbHVrrkS0R","outputId":"87c7f672-8363-4c28-c764-7b4a9a250788"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tweets in train: 9000\n","Tweets in validation: 1000\n","Tweets in test: 2970\n"]}],"source":["print(f\"Tweets in train: {train_data.shape[0]}\")\n","print(f\"Tweets in validation: {val_data.shape[0]}\")\n","print(f\"Tweets in test: {test_data.shape[0]}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GDarlbM0kS0S"},"source":["Split text and labels into seperate lists."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1367,"status":"ok","timestamp":1686743790354,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"dAkT-zQtkS0S"},"outputs":[],"source":["train_corpus = [tweet[\"text\"] for tweet in train_data]\n","train_labels = [tweet[\"label\"] for tweet in train_data]\n","test_corpus = [tweet[\"text\"] for tweet in test_data]\n","test_labels = [tweet[\"label\"] for tweet in test_data]\n","val_corpus = [tweet[\"text\"] for tweet in val_data]\n","val_labels = [tweet[\"label\"] for tweet in val_data]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HgyV6FT-kS0T"},"source":["Examine class distribution in the different splits."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686743790355,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"K0G0-CpokS0U","outputId":"f1feb449-d48f-4369-bf30-222198ab736d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Label distribtion in train: {'non-hate': 5217, 'hate': 3783}\n","Label distribtion in validataion: {'non-hate': 573, 'hate': 427}\n","Label distribtion in test: {'non-hate': 1718, 'hate': 1252}\n"]}],"source":["label_dict = {0:\"non-hate\", 1:\"hate\"}\n","\n","print(f\"Label distribtion in train: {dict(Counter([label_dict[label] for label in train_labels]))}\")\n","print(f\"Label distribtion in validataion: {dict(Counter([label_dict[label] for label in val_labels]))}\")\n","print(f\"Label distribtion in test: {dict(Counter([label_dict[label] for label in test_labels]))}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0--LLK_mkS0U"},"source":["Preprocessing the data. We note that to avoid an extensive process of providing accurate Part-of-Speech-tags to the lemmatizer we only stem the tokens, though lemmatization arguably might improve model performance."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686743790355,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"VEnddNPJkS0V"},"outputs":[],"source":["def preproc(_str):\n","    # remove twittertags + lowercase\n","    _str = re.sub(r'@\\w+', \"\", _str.lower())\n","    # remove numbers\n","    _str = re.sub(r'\\d+', \"\", _str)\n","    # remove punctuations\n","    _str = _str.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"!\",\"\")))\n","    # Remove extra whitespaces\n","    _str = re.sub(r'\\s+', ' ', _str.strip())\n","    # tokenize text - we do not use TweetTokenize as we have removed @ either way\n","    tokens = word_tokenize(_str)\n","    # remove stopwords and stem\n","    stemmer = PorterStemmer()\n","    tokens = [stemmer.stem(word) for word in tokens if word not in stopwords.words('english')]\n","\n","    return ' '.join(tokens) # join words back into a string"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":30957,"status":"ok","timestamp":1686743821309,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"ZqSCOtFekS0V"},"outputs":[],"source":["train_corpus_preproc = [preproc(tweet) for tweet in train_corpus]\n","test_corpus_preproc = [preproc(tweet) for tweet in test_corpus]\n","val_corpus_preproc = [preproc(tweet) for tweet in val_corpus]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8cJIW1O_kS0W"},"source":["**Considerations on the preprocessing**: As preprocessing is more of an assessment than a hard science, we have made some decisions that we think are best suited for the task at hand, i.e. to classify hate-speech on twitter. Some of these are:\n","- Retaining `!` when removing punctuation, as exclamations could be useful for the model when predicting hate-speech.\n","- Though capital letters could convey some meaning when predicting hate-speech, We have lower-cased all words, to make the classification more about semantic meaning than letter-capitalization.\n","\n","Ultimately, preprocessing data is about conveying as many relevant nuances as possible in the most simplified way. For instance, if stopwords do not provide any information on hate speech to the model, they should be removed. Should the model be used in production or published, we would have engaged in a process of trial-and-error, going back and forth between different preprocessing steps to see which preprocessing yields the best model performance.\n","\n","For information, the authors of the `TweetEval Hate Speech` dataset have already done a bit of text preprocessing, converting username, indicated by a leading `@`, into a standard `@username` token and convert all URLs, indicated by a leading `http` into a standard `http` token."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"io2za-w9kS0W"},"source":["Use `TfidfVectorizer()` to convert the documents into a TF-IDF feature matrix. We use both `unigrams` and `bigrams` to give the model a little more context about the context of the words - i.e. neighboring words."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":984,"status":"ok","timestamp":1686743822283,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"vmquDJgRkS0X"},"outputs":[],"source":["vectorizer = TfidfVectorizer(analyzer=\"word\",\n","                             ngram_range = (1,2))\n","\n","train_features_lr = vectorizer.fit_transform(train_corpus_preproc)\n","# only transform() on val and test, to make the evaluation resemble \"unseen data\" more\n","val_features_lr = vectorizer.transform(val_corpus_preproc)\n","test_features_lr = vectorizer.transform(test_corpus_preproc)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CqsFF42fkS0Y"},"source":["### Logistic regression with ridge regularization (with a c parameter tuned via cross-validation on the train set)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7RwUpuJekS0Y"},"source":["To optimize the hyperparameter - `C` - for regularizing the model to avoid overfitting, we carry out a `GridSearchCV`, with 5 folds and spanning `np.logspace(-2, 2, 50)` values of `C`. We use ridge-regression for regularization."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243848,"status":"ok","timestamp":1686744066129,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"YUJY3gFRkS0Y","outputId":"0eb2043a-940a-4be8-db7f-cd484b730d86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best cross-validation score: 0.77\n","Best parameters:  {'C': 39.06939937054613}\n","Best estimator:  LogisticRegression(C=39.06939937054613, max_iter=300, random_state=0)\n"]}],"source":["param_grid = {\"C\": np.logspace(-2, 2, 50)} # search parameters to optimize\n","\n","lr_grid = GridSearchCV(LogisticRegression(penalty=\"l2\", random_state=0, max_iter=300),\n","                        param_grid = param_grid,\n","                        cv=5,\n","                        n_jobs=-1,\n","                        scoring = \"accuracy\")\n","\n","lr_grid.fit(train_features_lr, train_labels)\n","\n","print(\"Best cross-validation score: {:.2f}\".format(lr_grid.best_score_))\n","print(\"Best parameters: \", lr_grid.best_params_)\n","print(\"Best estimator: \", lr_grid.best_estimator_)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"28Hnhdr8kS0b"},"source":["## Also fit at least one of the following models to try and predict the hate speech status of the tweets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qlPnsir0kS0b"},"source":["All of the following models take and *understands* sequential data inputs, as opposed to the logistic regression model, which used a TF-IDF matrix as input. We have to preprocess the input data differently, depending on whether word order matters. For instance, the word `not` is part of nltk's `stopwords.words('english')`-package. `not` can shift the meaning of a sentence completely, depending on its position in a sentence, but non-sequential models have no way of discerning which part of the sentence the `not` relates to. Sequential models, however, do. Thus, we will commit a much less extensive data cleaning process in the following preprocessing phase, and only lower-case the words. As previously mentioned, the authors of the dataset have already standardized user-tags and urls."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ORWuf1HEkS0c"},"source":["We want to represent each token through an embedding, rather than as a Tf-Idf-score. We use the pretrained embeddings from `glove-twitter-200` to embed the features in our vocabulary into a 200-dimensional vector space. One of the main advantages of this kind of text `vectorization` - for instance compared to one-hot encoding - is to reduced the dimensionality of the feature space and decrease the effect of the curse of dimensionality (Raschka & Mirjalili 2019:590). Compared to TF-IDF matrices, we also decrease the number of dimensions and avoid a sparse matrix of mainly zeros."]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":207820,"status":"ok","timestamp":1686744414833,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"tV62k_88kS0c"},"outputs":[],"source":["# load the pretrained embeddings (these can be used as the embedding argument in create_embedding_matrix)\n","glove = gensim.downloader.load('glove-twitter-200')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k4EG4Rz_kS0c"},"source":["First, we want to establish our total vocabulary, were we lowercase all tokens. We also include "]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":3930,"status":"ok","timestamp":1686744418761,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"Mv5W98KukS0d"},"outputs":[],"source":["# creating the full list of vocabulary in the tweet_eval data\n","total_vocabulary = set()\n","for tweet in train_corpus+val_corpus+test_corpus:\n","    tokens = word_tokenize(tweet)\n","    for t in tokens:\n","        total_vocabulary.add(t.lower()) # lower.case\n","total_vocabulary = sorted(list(total_vocabulary))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rYh9rxtUkS0d"},"source":["We include a padding-token, \"\", as the first entry, which will be the first row in the embedding matrix. We do this to later ensure that the input sequences - i.e. tokenized sentences - have the same length. "]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686744418761,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"hIa15EtXkS0d"},"outputs":[],"source":["total_vocabulary = [\"\"]+total_vocabulary"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pkPdKU83kS0d"},"source":["We create a embedding matrix with the dimension `vocabulary_length x embedding_dimension`. These 200-dimensional vectors will serve as representations of the features and be the input to the RNN and LSTM. Tokens that are not already in the pretrained embeddings will have all their values set to zero."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":384,"status":"ok","timestamp":1686745786405,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"hiat7XEZkS0d","outputId":"9fa89946-39f1-4d54-e9fb-1d6eee6b718a"},"outputs":[{"name":"stdout","output_type":"stream","text":["33.14 % of tokens are out of vocabulary\n"]}],"source":["def create_embedding_matrix(tokens, embedding):\n","    \"\"\"creates an embedding matrix from pre-trained embeddings for a new vocabulary. It also adds an extra vector\n","    of zeroes in row 0 to embed the padding token, and initializes missing tokens as vectors of 0s\"\"\"\n","    oov = set()\n","    size = embedding.vector_size\n","    # note the extra zero vector that will be used for padding\n","    embedding_matrix=np.zeros((len(tokens),size))\n","    c = 0\n","    for i in range(1,len(tokens)):\n","        try:\n","            embedding_matrix[i]=embedding[tokens[i]]\n","        except KeyError: #to catch the words missing in the embeddings\n","            try:\n","                embedding_matrix[i]=embedding[tokens[i].lower()]\n","            except KeyError:\n","                #if the token does not have an embedding, we initialize it as a vector of 0s\n","                embedding_matrix[i] = np.zeros(size)\n","                #we keep track of the out of vocabulary tokens\n","                oov.add(tokens[i])\n","                c +=1\n","    print(f'{c/len(tokens)*100:.2f} % of tokens are out of vocabulary')\n","    return embedding_matrix, oov\n","\n","embedding_matrix, oov = create_embedding_matrix(total_vocabulary, glove)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JJCRvYzRkS0e"},"source":["Convert the text sentences into a list of indices, each of which corresponds to the words' indices in the total vocabulary and in the embedding matrix. Also, we use padding on the feature vectors to ensure that all vector representations of the input sentences have the same length."]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":92369,"status":"ok","timestamp":1686744511499,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"qspo3yIEkS0e"},"outputs":[],"source":["def text_to_indices(text, total_vocabulary):\n","    \"\"\"turns the input text into a vector of indices in total_vocabulary that corresponds to the tokenized words in the input text\"\"\"\n","    encoded_text = []\n","    tokens = word_tokenize(text)\n","    for t in tokens:\n","        index = total_vocabulary.index(t.lower())\n","        encoded_text.append(index)\n","    return encoded_text\n","\n","def add_padding(vector, max_length, padding_index):\n","    \"\"\"adds copies of the padding token to make the input vector the max_length size, so that all inputs are the same length\n","    (the length of tweet with most words)\"\"\"\n","    if len(vector) < max_length:\n","        vector = [padding_index for _ in range(max_length-len(vector))] + vector\n","    return vector\n","\n","# vectorize sentences to indices\n","train_features = [text_to_indices(text, total_vocabulary) for text in train_corpus]\n","val_features = [text_to_indices(text, total_vocabulary) for text in val_corpus]\n","test_features = [text_to_indices(text, total_vocabulary) for text in test_corpus]\n","\n","# Find the length of the longest tweet\n","longest_tweet = max(train_features+val_features+test_features, key=len)\n","max_length = len(longest_tweet)\n","padding_index = 0 #position 0 is where we had put the padding token in our vocabulary and embedding matrix\n","\n","# padding the feature vectors\n","train_features = [add_padding(vector, max_length, padding_index) for vector in train_features]\n","val_features = [add_padding(vector, max_length, padding_index) for vector in val_features]\n","test_features = [add_padding(vector, max_length, padding_index) for vector in test_features]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7hVz376FkS0k"},"source":["Next, we convert all splits of the data into a PyTorch DataSet as well as a DataLoader, to perform `mini batch gradient descent` when training the model."]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686744511500,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"qI_ukFHukS0l"},"outputs":[],"source":["class TweetEvalTrain(torch.utils.data.Dataset):\n","    # defining the sources of the data\n","    def __init__(self, features, labels):\n","        self.X = torch.LongTensor(features)\n","        self.y = torch.from_numpy(np.array(labels))\n","\n","    def __getitem__(self, index):\n","        X = self.X[index]\n","        y = self.y[index].unsqueeze(0)\n","        return X, y\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","data_train = TweetEvalTrain(train_features, train_labels)\n","data_val = TweetEvalTrain(val_features, val_labels)\n","data_test = TweetEvalTrain(test_features, test_labels)\n","\n","train_loader = torch.utils.data.DataLoader(data_train, batch_size=64)\n","val_loader = torch.utils.data.DataLoader(data_val, batch_size=64)\n","test_loader = torch.utils.data.DataLoader(data_test, batch_size=64)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dJNgsgTfkS0l"},"source":["The data is now ready to be fed into the RNN and LSTM."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5GJ7GZcpkS0m"},"source":["### Many-to-one RNN with pre-trained word embeddings as the inputs"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"R9Jefv2OkS0m"},"source":["First, we define both a custom RNN/LSTM class, a training_loop function and an evaluation function. Note that we do not need to explicitly define a softmax-activation for the outputs to be passed through, as our loss function, `CrossEntropyLoss`, automatically applies a softmax transformation before calculating the loss."]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686744511500,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"jHxzdMI4kS0n"},"outputs":[],"source":["# defining the embedding step and RNN model\n","\n","class RNN_LSTM(torch.nn.Module):\n","    def __init__(self, rnn_size, n_classes, embedding_matrix, num_layers=1, type_=\"RNN\"):\n","        super().__init__()\n","\n","        #applying the embeddings to the inputs. Tokens corresponding to the padding_idx will not be included in the training of the model\n","        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n","        emb_dim = embedding_matrix.shape[1] # Each word is represented through a 200 dimensional vector\n","        self.num_layers = num_layers\n","        self.type = type_\n","\n","        if self.type == \"RNN\":\n","            self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=self.num_layers, batch_first=True)\n","        elif self.type == \"LSTM\":\n","            self.rnn = torch.nn.LSTM(input_size=emb_dim, hidden_size=rnn_size, bidirectional=False, num_layers=self.num_layers, batch_first=True)\n","        else:\n","            raise LookupError(\"Only RNN and LSTM are supported.\")\n","\n","        #applies a linear transformation to the RNN/LSTM\n","        self.linear_outputs = torch.nn.Linear(rnn_size, n_classes)\n","\n","    def forward(self, inputs):\n","        # encode the input vectors\n","        encoded_inputs = self.embedding(inputs)\n","\n","        # In this many-to-one model, we only need the final hidden states,\n","        # where all prio information is somehow present\n","        if self.type == \"RNN\" and self.num_layers > 1:\n","            all_states, final_state = self.rnn(encoded_inputs)\n","            outputs = self.linear_outputs(final_state[-1,:,:]) # only last layer as it is a deep RNN\n","\n","        elif self.type == \"RNN\" and self.num_layers == 1:\n","            all_states, final_state = self.rnn(encoded_inputs)\n","            outputs = self.linear_outputs(final_state.squeeze())\n","\n","        elif self.type == \"LSTM\" and self.num_layers == 1:\n","            all_states, (final_state, c_n) = self.rnn(encoded_inputs)\n","            outputs = self.linear_outputs(final_state.squeeze())\n","\n","        return outputs\n","\n","# training loop\n","def training_loop(model, num_epochs):\n","    loss_function = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(num_epochs):\n","        losses = []\n","        for batch_index, (inputs, targets) in enumerate(train_loader):\n","            optimizer.zero_grad() # zero the gradients from the previous opt step\n","            outputs = model.forward(inputs).squeeze() # compute outputs\n","            targets = targets.squeeze().long()\n","            loss = loss_function(outputs, targets) #loss function\n","            loss.backward() # backpropagation - get deriv\n","            optimizer.step() # optimize based on derivative\n","            losses.append(loss.item()) # append batch loss\n","        print(f'Epoch {epoch+1}: loss {np.mean(losses)}')\n","    return model\n","\n","def evaluate(model, val_loader):\n","    predictions = []\n","    labels = []\n","    with torch.no_grad(): # don't backpropagate or update weights anymore\n","        for batch_index, (inputs, targets) in enumerate(val_loader):\n","            # apply softmax\n","            outputs = torch.softmax(model(inputs), 1 )\n","            # indices highest softmax values\n","            vals, indices = torch.max(outputs, 1)\n","            # accumulating the predictions\n","            predictions += indices.tolist()\n","            # accumulating true labels\n","            labels += targets.tolist()\n","\n","    acc = accuracy_score(predictions, labels)\n","    f1 = f1_score(predictions, labels)\n","    print(f'Model accuracy: {acc:.2f}')\n","    print(f'Model F1: {f1:.2f}')\n","    return acc, f1, predictions"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FcWRBKuJkS0n"},"source":["We start by training a RNN with a single hidden layer and 100 nodes in the layer."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36610,"status":"ok","timestamp":1686744548107,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"PrdH5uULkS0o","outputId":"0c2a7c42-a739-4efd-d972-8e790d23d50b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1: loss 0.6058707241470932\n","Epoch 2: loss 0.550754392189337\n","Epoch 3: loss 0.5269700645977724\n","Epoch 4: loss 0.5149752362400082\n","Epoch 5: loss 0.5122840057873557\n","Model accuracy: 0.68\n","Model F1: 0.66\n"]}],"source":["# initializing and training the model:\n","myRNN = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 1, embedding_matrix=embedding_matrix)\n","\n","myRNN = training_loop(myRNN, num_epochs = 5)\n","acc, f1, preds = evaluate(myRNN, val_loader)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RJxU-J4KkS0p"},"source":["\n","### LSTM"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cjLnPY86kS0p"},"source":["A single hidden layer LSTM with 100 nodes in the layer."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80990,"status":"ok","timestamp":1686745006540,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"J5eELmlEkS0p","outputId":"7c0318d4-4b9e-46cd-8dfc-9ccf30d19e3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1: loss 0.5880376453518023\n","Epoch 2: loss 0.5178072105908225\n","Epoch 3: loss 0.4880447679377617\n","Model accuracy: 0.72\n","Model F1: 0.64\n"]}],"source":["myLSTM = RNN_LSTM(rnn_size=100, n_classes=2, num_layers = 1, embedding_matrix=embedding_matrix, type_=\"LSTM\")\n","\n","myLSTM = training_loop(myLSTM, num_epochs = 3)\n","acc, f1, preds = evaluate(myLSTM, val_loader)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CFnDyB3qkS0r"},"source":["### A fine-tuned BERT model"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":806,"status":"ok","timestamp":1686745007335,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"Z50orgpZQGoQ"},"outputs":[],"source":["metric_f1 = load_metric(\"f1\")#use the load_metric method from the datasets library to load f1 from sklearn\n","metric_acc = load_metric(\"accuracy\")\n","\n","# Defining a function that compute metrics and give a tuple of outputs and labels\n","def compute_metrics(eval_pred):\n","      outputs, labels = eval_pred\n","      predictions = np.argmax(outputs, axis=-1)\n","      f1 = metric_f1.compute(predictions=predictions, references=labels)\n","      acc = metric_acc.compute(predictions=predictions, references=labels)\n","      return f1 | acc\n","\n","def BERT_hate_classifier(model_name):\n","  # allow model to access the GPU\n","  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","  # Set up the tokenizer we want to use\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","  # Moving tokenizer to work on GPU\n","  tokenizer.to_device = device\n","  # Apply the tokenizer to each row in the dataset\n","  tokenized_train_dataset = train_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n","  tokenized_val_dataset = val_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n","  tokenized_test_dataset = test_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n","  ## Specify task for pretrained model\n","  hate_classifier = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2)\n","  # Moving model to GPU\n","  hate_classifier.to(device)\n","  # Specify training parameters\n","  training_args = TrainingArguments(output_dir=\"bert_hatespeech\",\n","                                    evaluation_strategy = \"steps\",\n","                                    num_train_epochs=5,\n","                                    per_device_train_batch_size=16,\n","                                    logging_steps=500,\n","                                    eval_steps=500)\n","\n","  trainer = Trainer(model = hate_classifier,\n","                    args = training_args,\n","                    compute_metrics = compute_metrics,\n","                    train_dataset = tokenized_train_dataset,\n","                    eval_dataset = tokenized_val_dataset,\n","                    tokenizer = tokenizer)\n","  # fine-tune model\n","  trainer.train()\n","\n","  return trainer, tokenized_test_dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4ORn4xWWTKsW"},"source":[" We train a BERT model, that is not specialized in twitter hate speech, but a more general purpose BERT trained on Wikipedia and The Book Corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvrgZBYdfuUn"},"outputs":[],"source":["trainer, tokenized_test_dataset=  BERT_hate_classifier(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1686743696419,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"Q6hHC1beSXhQ","outputId":"f247f22d-1384-43bf-bc3b-b15bc7d3278e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of fine-tuned BERT model: 0.77\n","F1 of fine-tuned BERT model: 0.74\n"]}],"source":["eval_on_val_data = trainer.evaluate()\n","\n","print(f\"Accuracy of fine-tuned BERT model: {eval_on_val_data['eval_accuracy']:.2f}\")\n","print(f\"F1 of fine-tuned BERT model: {eval_on_val_data['eval_f1']:.2f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YT0qBmJQwBd0"},"source":["Next, to see how much of a difference the pre-trained BERT model makes, we also train a BERT that is developed by the authors behind the TweetEval Hate Speech dataset, and is pre-trained to detect hate speech on twitter (Basile et al. 2019)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjJp97FSv5rS"},"outputs":[],"source":["trainer_tweeteval, tokenized_test_dataset_tweeteval =  BERT_hate_classifier(\"cardiffnlp/twitter-roberta-base-hate-latest\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1686743706931,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"GUd7hTQtv73u","outputId":"dd94650e-ee1e-4fd2-cd99-8f4a2ddc911c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of fine-tuned BERT model: 0.81\n","F1 of fine-tuned BERT model: 0.79\n"]}],"source":["eval_on_val_data = trainer_tweeteval.evaluate()\n","\n","print(f\"Accuracy of fine-tuned BERT model: {eval_on_val_data['eval_accuracy']:.2f}\")\n","print(f\"F1 of fine-tuned BERT model: {eval_on_val_data['eval_f1']:.2f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cG4sF1CLkS0w"},"source":["## For each of the models you ran in question 1. and 2., briefly discuss (two–four sentences) in what ways the model is a good choice for the current task and data set, plus any downsides the model might have for this application.\n","\n","**Logistic Regression**: Since Logistic Regression uses a TF-IDF vectorization to convert words and sentences into numbers, it disregards the order with which the words appear in the sentences. That word ordering does not matter to the meaning of a sentence is a somewhat crude assumption, though with regard to this particular task of detecting hate speech, simply identifying the presence or prevalence of \"hateful word\" could suffice.\n","\n","**RNN and LSTM**: The fact that both RNN and LSTM have a sequential architecture makes them rather fitting for modeling the semantic meaning of language, compared to the TF-IDF vectorization of the Logistic Regression. The LSTM tries to mitigate the risk of exploding/vanishing gradients that the RNN might suffer from when the input sequence is long. Thus, LSTMs are better when it comes to longer sentences, as the core concept of an LSTM is to navigate what to remember from the previous parts of the sequence, and what to forget. However, tweets are characterized by being short sentences with a limit of 280 characters, and our dataset might not have the proper sentence lengths for the LSTM architecture to fully flourish, compared to the RNN and Logistic Regression.\n","\n","Using pretrained embeddings for the text vectorization as we did with RNN and LSTM, the performances of our models are influenced by the corpus used to construct the n-dimensional embedding vector space. For instance in relation to inheriting biases in the embeddings stemming from the data on which it was trained, or if the vocabulary of the embeddings is small, wherefor many of the tokens from our own dataset might not be \"embed-able\". We saw that around 30% of the tokens were not in the pretrained vocabulary of GloVe, so almost a third of our vocabulary will not be used to train the model or when predicting.\n","\n","**BERT**: As with RNN and LSTM, BERT models allow for sequential input, which is fitting for language classification. The fine-tuned BERT model had the best performance, both with `accuracy` and `F1`. We used two pre-trained BERT-models; One that is trained on twitter data with the objective to detect hate speech, and one that is trained for more general language processing. The model specialized on twitter hate speech is very convenient for our task at hand, but it also gives the model a natural head-start compared to some of the other models, where we used general, pretrained word-embeddings or simple preprocessing/TF-IDF. In our case, the relatively better performance of the BERT models thus depends on the existence of a pre-trained model, and also what that model is trained for. Alternatively, we would have had to pre-train the BERT-model from scratch, which would have been a much more extensive task."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NzkqwuB1kS0x"},"source":["## Present the performance of your models on the test set in terms of both F1 and accuracy. Which is the best-performing model in terms of F1? In your opinion, should we prefer F1 over accuracy as an indicator of model performance here, and why?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kQ_9Tj87E6kn"},"source":["First, we calculate the `accuracy` and `F1` score for each model on the `test data`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P4M5PxppzK3H"},"outputs":[],"source":["test_pred_lr = lr_grid.predict(test_features_lr) # LLR\n","acc_rnn, f1_rnn, preds = evaluate(myRNN, test_loader) # RNN\n","acc_deep_rnn, f1_deep_rnn, preds = evaluate(my_deep_RNN, test_loader) # Deep RNN\n","acc_lstm, f1_lstm, preds = evaluate(myLSTM, test_loader) # LSTM\n","\n","output, true_labels, eval = trainer.predict(tokenized_test_dataset) # BERT\n","acc_bert, f1_bert = eval[\"test_accuracy\"], eval[\"test_f1\"]\n","\n","output, true_labels, eval_tweeteval = trainer_tweeteval.predict(tokenized_test_dataset_tweeteval) # TweetEval BERT\n","acc_bert_tweeteval, f1_bert_tweeteval = eval_tweeteval[\"test_accuracy\"], eval_tweeteval[\"test_f1\"]"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"elapsed":361,"status":"ok","timestamp":1686745076662,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"np3kK87wvaAi","outputId":"3a5e99de-3eab-412f-e446-b6f869c24cea"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c51f7208-625d-47aa-afaa-e97f716721e2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Logistic Regression</th>\n","      <td>0.49</td>\n","      <td>0.61</td>\n","    </tr>\n","    <tr>\n","      <th>RNN</th>\n","      <td>0.48</td>\n","      <td>0.58</td>\n","    </tr>\n","    <tr>\n","      <th>Deep RNN</th>\n","      <td>0.52</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>LSTM</th>\n","      <td>0.53</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>BERT TweetEval</th>\n","      <td>0.57</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>BERT</th>\n","      <td>0.53</td>\n","      <td>0.64</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c51f7208-625d-47aa-afaa-e97f716721e2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c51f7208-625d-47aa-afaa-e97f716721e2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c51f7208-625d-47aa-afaa-e97f716721e2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                     Accuracy    F1\n","Logistic Regression      0.49  0.61\n","RNN                      0.48  0.58\n","Deep RNN                 0.52  0.60\n","LSTM                     0.53  0.60\n","BERT TweetEval           0.57  0.65\n","BERT                     0.53  0.64"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame([[accuracy_score(test_pred_lr, test_labels),\n","               f1_score(test_pred_lr, test_labels)],\n","              [acc_rnn, f1_rnn],\n","              [acc_deep_rnn, f1_deep_rnn],\n","              [acc_lstm, f1_lstm],\n","              [acc_bert_tweeteval, f1_bert_tweeteval],\n","              [acc_bert, f1_bert]],\n","             index = [\"Logistic Regression\", \"RNN\", \"Deep RNN\", \"LSTM\", \"BERT TweetEval\", \"BERT\"],\n","             columns = [\"Accuracy\", \"F1\"]).round(2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### NYT FORSLAG\n","First of all, we notice that all models perform significantly worse on the test data, compared to their performance on the validation data. This could obviously be a modeling issue, but it could also encourage a closer look at the test datasets, to see if it comes down to an issue of noise or messy data. The results, however, are close to the one's measured by the authors of the `TweetEval Hate Speech` dataset (Basile et al. 2019).\n","\n","Deciding how to weigh the importance of either the `accuracy` or `F1` depends on the task and what one wants to achieve - or not to achieve - with one's classification model. Accuracy can serve as an appropriate performance metric along with its straightforward interpretability, i.e. how many hate or non-hate tweets did the classifier predict correctly. It serves as a good indicator especially when the outcomes in the dataset are reasonably balanced. The `F1-score`, on the other hand, is less intuitively interpretable, but is a better pick, if the classes in the dataset are imbalanced. This is because that the `F1-score` depend on `False Positives` and `False Negatives` - i.e. if the model incorrectly classifies something as hate speech or if it fails to classify actual hate speech Both these types of errors are assessed relative to the number of True Positives, revealing how the model handles different forms of misclassification (Hovy 2022b: 25f). As we are trying to predict hate speech in twitter data we are mostly interested in getting a model with a generally good score with accuracy and F1, but if one were trying to predict on something in which False Negatives or False Positives would be catastrophic then one could tweak the model to take this into consideration. \n","\n","In our case with the twitter-eval speech data the class-balance between `hate` and `non-hate` is fairly evenly distributed, and thus the full benefits of the F1 score is not fully pronounced. However adhering to best practice, we present both metrics to provide a comprehensive evaluation of our model´s performance  \n","\n","Also, whether various forms of preprocessing could have increased model performance would have to be explored if one were to further fine-tune the models for production or publication."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mPXFnMYSkS0z"},"source":["### DET GAMLE TEKST\n","First of all, we notice that all models perform significantly worse on the test data, compared to their performance on the validation data. This could obviously be a modeling issue, but it could also encourage a closer look at the test datasets, to see if it comes down to an issue of noise or messy data. The results, however, are close to the one's measured by the authors of the `TweetEval Hate Speech` dataset (Basile et al. 2019).\n","\n","Deciding how to weigh the importance of either the `accuracy` or `F1` depends on the task and what one wants to achieve - or not to achieve - with one's classification model. As previously presented, the class-balance between `hate` and `non-hate` is fairly evenly distributed, and accuracy can then serve as an appropriate performance metric along with its straightforward interpretability, i.e. how many tweets did the classifier predict correctly.\n","\n","The `F1-score`, on the other hand, is less intuitively interpretable, but would likely have been a better pick, had the classes been more imbalanced (Hovy 2022b: 25f). The F1 score also cares about how the classes have been numerically encoded, meaning which class is a `Positive` and which is a `Negative`. In our case, we have assigned hate speech to be positives, i.e having a value of one. That is because the `F1-score` depend on `False Positives` and `False Negatives` - i.e. if the model incorrectly classifies something as hate speech or if it fails to classify hate speech as such - both in relation to how many `True Positives` the model have classified. Thus, F1-score cares about which type of error the model makes.\n","\n","Also, whether various forms of preprocessing could have increased model performance would have to be explored if one were to further fine-tune the models for production or publication."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"afI5KdCRkS00"},"source":["## Based on the paper by Basile et al. (2019), What further info might you have liked to have about the data selection process and/or the annotation process for this data set? Why?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_REAg-dn8bSU"},"source":["Hate Speech is an abstract and arguably ambiguous concept, and human labeling is not necessarily a highly systematic and reliable endeavor.\n","\n","With regard to the **data selection process**, the term `identified hater` is not further defined (Basile et al. 2019: 55). Thus, the exact sampling strategy is unclear.\n","\n","In the **annotation process**, the authors try to counter the potential inconsistency of the individual annotator by using majority voting on each tweet among three annotators (crowds). The tweet is also annotated by two experts, whereafter the final label is a majority voting between crowd, expert 1, and expert 2. The exact qualifications of the experts remains unclear in the article, aside from language capability and general field knowledge. The article provides a measure of the average confidence (AC) between the annotations, but it is unclear if this measure only relates to the crowd. It would have been informative to explicitly have both the AC between the crowd and between the crowd and the experts. Though the aggregate AC measure is informative, it could have been interesting to include a measure of agreement/reliability for each tweet in the dataset, as to get an indication of which tweets have been unanimously annotated and which one's have been more ambiguous annotated.\n","\n","The authors should explicitly outline their approach to safeguarding the working conditions of their crowdsourced labor. Upholding rigorous ethical guidelines in scientific research necessitates clarity about the measures taken to ensure fair pay and appropriate working conditions, especially when crowdsourcing has a history of being abusive of its workers. As a general discursive consideration, it could also be interesting to know more about the social characteristics of the annotators and experts. Are they all from the same country or do they all identify as the same gender? There might be discursive reminiscents of these social dimensions latently embedded in the annotation of what is hate speech and what is not in the data. \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gvm_xNm3JGfm"},"source":["## Literature\n","\n","**Basile, V., Bosco, C., Fersini, E., Nozza, D., Patti, V., Rangel Pardo, F. M., Rosso, P., & Sanguinetti, M.** (2019). SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter. In *Proceedings of the 13th International Workshop on Semantic Evaluation* (pp. 54-63). Minneapolis, Minnesota, USA: Association for Computational Linguistics. [Link](https://www.aclweb.org/anthology/S19-2007) (DOI: [10.18653/v1/S19-2007](https://doi.org/10.18653/v1/S19-2007))\n","\n","**Raschka, Sebastian, V. Mirjalili** (2019). Python Machine Learning;Machine Learning and Deep Learning with Python Scikit-Learn and Tensorflow 2 3rd Edition. https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=2329991. Accessed June 14 2023.\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
