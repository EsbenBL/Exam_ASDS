{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"LUtlvdadkS0D"},"source":["# Advanced Social Data Science 2\n","\n","*By Carl, Asger, & Esben*"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1686728066849,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"bJdrKTFxksPS"},"outputs":[],"source":["# !pip install datasets==2.2.1 transformers==4.19.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17580,"status":"ok","timestamp":1686739356555,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"HGlKMLJVkS0J","outputId":"55f81027-e072-4168-c1c5-ec47086beb69"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import string\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, f1_score\n","import gensim.downloader\n","import torch\n","import re\n","from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n","from datasets import load_metric, load_dataset\n","import numpy as np\n","from google.colab import drive\n","import nltk\n","import pandas as pd\n","import gensim.downloader\n","import pandas as pd\n","import numpy as np\n","import torch\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aLNZQNEBkS0L"},"source":["# Part 3 Supervised text classification\n","\n","In this section, we will train a range of models to predict whether a tweet could be classified as hate speech or not. We will use a ridge regularized Logistic Regression, a Recurrent Neural Network (RNN), a Long Short-Term Memomry model (LSTM), and two BERT models with different pre-trainings - a general purpose BERT and a BERT specialized to detect hate speech on Twitter.  "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zAkxIs8JkS0M"},"source":["## Fitting a logistic regression to predict the hate speech status of the tweets, using TF-IDF features."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p5AkzQcGkS0N"},"source":["First, we load the `tweet_eval` dataset and split the data into train, validation, and test. In the data `0` signifies non-hate and `1` indicates hate."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1352,"status":"ok","timestamp":1686743788988,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"bZRqtUdJkS0O","outputId":"feb0db19-b532-486b-bb02-bf0da9b08bd8"},"outputs":[],"source":["train_data=load_dataset(\"tweet_eval\", \"hate\", split=\"train\")\n","val_data=load_dataset(\"tweet_eval\", \"hate\", split=\"validation\")\n","test_data=load_dataset(\"tweet_eval\", \"hate\", split=\"test\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GDarlbM0kS0S"},"source":["Split text and labels into seperate lists."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1367,"status":"ok","timestamp":1686743790354,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"dAkT-zQtkS0S"},"outputs":[],"source":["train_corpus=[tweet[\"text\"] for tweet in train_data]\n","train_labels=[tweet[\"label\"] for tweet in train_data]\n","test_corpus=[tweet[\"text\"] for tweet in test_data]\n","test_labels=[tweet[\"label\"] for tweet in test_data]\n","val_corpus=[tweet[\"text\"] for tweet in val_data]\n","val_labels=[tweet[\"label\"] for tweet in val_data]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HgyV6FT-kS0T"},"source":["Examine number of observations and class distribution in the different splits."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686743790355,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"K0G0-CpokS0U","outputId":"f1feb449-d48f-4369-bf30-222198ab736d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Label distribtion in train: {'non-hate': 5217, 'hate': 3783}\n","Label distribtion in validataion: {'non-hate': 573, 'hate': 427}\n","Label distribtion in test: {'non-hate': 1718, 'hate': 1252}\n"]}],"source":["label_dict={0:\"non-hate\", 1:\"hate\"}\n","\n","print(f\"Label distribtion in train: {dict(Counter([label_dict[label] for label in train_labels]))}\")\n","print(f\"Label distribtion in validataion: {dict(Counter([label_dict[label] for label in val_labels]))}\")\n","print(f\"Label distribtion in test: {dict(Counter([label_dict[label] for label in test_labels]))}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0--LLK_mkS0U"},"source":["Preprocessing the data. The authors of the `TweetEval Hate Speech` dataset have already done a bit of text preprocessing, converting username, indicated by a leading `@`, into a standard `@username` token and convert all URLs, indicated by a leading `http` into a standard `http` token."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686743790355,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"VEnddNPJkS0V"},"outputs":[],"source":["def preproc(_str):\n","    # remove twittertags + lowercase\n","    _str=re.sub(r'@\\w+', \"\", _str.lower())\n","    # remove numbers\n","    _str=re.sub(r'\\d+', \"\", _str)\n","    # remove punctuations\n","    _str=_str.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"!\",\"\")))\n","    # Remove extra whitespaces\n","    _str=re.sub(r'\\s+', ' ', _str.strip())\n","    # tokenize text - we do not use TweetTokenize as we have removed @ either way\n","    tokens=word_tokenize(_str)\n","    # remove stopwords and stem\n","    stemmer=PorterStemmer()\n","    tokens=[stemmer.stem(word) for word in tokens if word not in stopwords.words('english')]\n","\n","    return ' '.join(tokens) # join words back into a string"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":30957,"status":"ok","timestamp":1686743821309,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"ZqSCOtFekS0V"},"outputs":[],"source":["train_corpus_preproc=[preproc(tweet) for tweet in train_corpus]\n","test_corpus_preproc=[preproc(tweet) for tweet in test_corpus]\n","val_corpus_preproc=[preproc(tweet) for tweet in val_corpus]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8cJIW1O_kS0W"},"source":["As preprocessing is more of an assessment than a hard science, we have made some decisions that we think are best suited for the task at hand, i.e. to classify hate-speech on twitter. Some of these are:\n","- Retaining `!` when removing punctuation, as exclamations could be useful for the model when predicting hate-speech.\n","- Though capital letters could convey some meaning when predicting hate-speech, We have lower-cased all words, to make the classification more about semantic meaning than letter-capitalization.\n","- We only stem the tokens, though lemmatization arguably might improve model performance with proper Part-of-speech tags.\n","\n","Ultimately, preprocessing data is about conveying as many relevant nuances as possible in the most simplified way. For instance, if stopwords do not provide any information on hate speech to the model, they should be removed. Should the model be used in production or published, we would have engaged in a process of trial-and-error, going back and forth between different preprocessing steps to see which preprocessing yields the best model performance."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"io2za-w9kS0W"},"source":["Convert the documents into a TF-IDF feature matrix. We use both `unigrams` and `bigrams` to give the model a little more context about the context of the words - i.e. neighboring words."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":984,"status":"ok","timestamp":1686743822283,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"vmquDJgRkS0X"},"outputs":[],"source":["vectorizer=TfidfVectorizer(analyzer=\"word\",\n","                             ngram_range=(1,2))\n","\n","train_features_lr=vectorizer.fit_transform(train_corpus_preproc)\n","# only transform() on val and test, to make the evaluation resemble \"unseen data\" more\n","val_features_lr=vectorizer.transform(val_corpus_preproc)\n","test_features_lr=vectorizer.transform(test_corpus_preproc)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CqsFF42fkS0Y"},"source":["### Logistic regression with ridge regularization\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7RwUpuJekS0Y"},"source":["To optimize the hyperparameter - `C` - for regularizing the model to avoid overfitting, we carry out a `GridSearchCV`, with 5 folds and spanning `np.logspace(-2, 2, 50)` values of `C`. We use ridge-regression for regularization."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243848,"status":"ok","timestamp":1686744066129,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"YUJY3gFRkS0Y","outputId":"0eb2043a-940a-4be8-db7f-cd484b730d86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best cross-validation score: 0.77\n","Best parameters:  {'C': 39.06939937054613}\n","Best estimator:  LogisticRegression(C=39.06939937054613, max_iter=300, random_state=0)\n"]}],"source":["param_grid={\"C\": np.logspace(-2, 2, 50)} # search parameters to optimize\n","\n","lr_grid=GridSearchCV(LogisticRegression(penalty=\"l2\", random_state=0, max_iter=300),\n","                        param_grid=param_grid,\n","                        cv=5,\n","                        n_jobs=-1,\n","                        scoring=\"accuracy\")\n","\n","lr_grid.fit(train_features_lr, train_labels)\n","\n","print(\"Best cross-validation score: {:.2f}\".format(lr_grid.best_score_))\n","print(\"Best parameters: \", lr_grid.best_params_)\n","print(\"Best estimator: \", lr_grid.best_estimator_)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"28Hnhdr8kS0b"},"source":["## Fitting an RNN, LSTM and two BERT-models to predict the hate speech status of the tweets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qlPnsir0kS0b"},"source":["All of the following models take and *understands* sequential data inputs, as opposed to the logistic regression model, which used a TF-IDF matrix as input. We have to preprocess the input data differently, depending on whether word order matters. For instance, the word `not` is part of nltk's `stopwords.words('english')`-package. `not` can shift the meaning of a sentence completely, depending on its position in a sentence, but non-sequential models have no way of discerning which part of the sentence the `not` relates to. Sequential models, however, do. Thus, we will commit a much less extensive data cleaning process in the following preprocessing phase, and only lower-case the words."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ORWuf1HEkS0c"},"source":["We use the pretrained embeddings from `glove-twitter-200` to embed the features in our vocabulary into a 200-dimensional vector space. One of the main advantages of this kind of text `vectorization` - for instance compared to one-hot encoding - is to reduced the dimensionality of the feature space and decrease the effect of the curse of dimensionality (Raschka & Mirjalili 2019:590). "]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":207820,"status":"ok","timestamp":1686744414833,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"tV62k_88kS0c"},"outputs":[],"source":["# pretrained embeddings\n","glove=gensim.downloader.load('glove-twitter-200')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k4EG4Rz_kS0c"},"source":["We establish our total vocabulary and lowercase all tokens."]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":3930,"status":"ok","timestamp":1686744418761,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"Mv5W98KukS0d"},"outputs":[],"source":["# creating the full list of vocabulary in the tweet_eval data\n","total_vocabulary=set()\n","for tweet in train_corpus+val_corpus+test_corpus:\n","    tokens=word_tokenize(tweet)\n","    for t in tokens:\n","        total_vocabulary.add(t.lower()) # lower.case\n","total_vocabulary=sorted(list(total_vocabulary))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rYh9rxtUkS0d"},"source":["We include a padding-token, \"\", which will be the first row in the embedding matrix to later ensure that the input sequences - i.e. tokenized sentences - have the same length. "]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686744418761,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"hIa15EtXkS0d"},"outputs":[],"source":["total_vocabulary=[\"\"]+total_vocabulary"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pkPdKU83kS0d"},"source":["We create an embedding matrix with the dimension `vocabulary_length x embedding_dimension`. These 200-dimensional vectors will serve as representations of the features and be the input to the RNN and LSTM. Tokens that are not already in the pretrained embeddings will have all their values set to zero."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":384,"status":"ok","timestamp":1686745786405,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"hiat7XEZkS0d","outputId":"9fa89946-39f1-4d54-e9fb-1d6eee6b718a"},"outputs":[{"name":"stdout","output_type":"stream","text":["33.14 % of tokens are out of vocabulary\n"]}],"source":["def create_embedding_matrix(tokens, embedding):\n","    oov=set()\n","    size=embedding.vector_size\n","    # note the extra zero vector that will be used for padding\n","    embedding_matrix=np.zeros((len(tokens),size))\n","    c=0\n","    for i in range(1,len(tokens)):\n","        try:\n","            embedding_matrix[i]=embedding[tokens[i]]\n","        except KeyError: #to catch the words missing in the embeddings\n","            try:\n","                embedding_matrix[i]=embedding[tokens[i].lower()]\n","            except KeyError:\n","                #if the token does not have an embedding, we initialize it as a vector of 0s\n","                embedding_matrix[i]=np.zeros(size)\n","                #we keep track of the out of vocabulary tokens\n","                oov.add(tokens[i])\n","                c +=1\n","    print(f'{c/len(tokens)*100:.2f} % of tokens are out of vocabulary')\n","    return embedding_matrix, oov\n","\n","embedding_matrix, oov=create_embedding_matrix(total_vocabulary, glove)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JJCRvYzRkS0e"},"source":["Convert the text sentences into a list of indices, each of which corresponds to the words' indices in the total vocabulary and in the embedding matrix. Also, we use padding on the feature vectors to ensure that all vector representations of the input sentences have the same length."]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":92369,"status":"ok","timestamp":1686744511499,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"qspo3yIEkS0e"},"outputs":[],"source":["def text_to_indices(text, total_vocabulary):\n","    \"\"\"turns the input text into a vector of indices in total_vocabulary that corresponds to the tokenized words in the input text\"\"\"\n","    encoded_text=[]\n","    tokens=word_tokenize(text)\n","    for t in tokens:\n","        index=total_vocabulary.index(t.lower())\n","        encoded_text.append(index)\n","    return encoded_text\n","\n","def add_padding(vector, max_length, padding_index):\n","    \"\"\"adds copies of the padding token to make the input vector the max_length size, so that all inputs are the same length\n","    (the length of tweet with most words)\"\"\"\n","    if len(vector) < max_length:\n","        vector=[padding_index for _ in range(max_length-len(vector))] + vector\n","    return vector\n","\n","# vectorize sentences to indices\n","train_features=[text_to_indices(text, total_vocabulary) for text in train_corpus]\n","val_features=[text_to_indices(text, total_vocabulary) for text in val_corpus]\n","test_features=[text_to_indices(text, total_vocabulary) for text in test_corpus]\n","\n","# Find the length of the longest tweet\n","longest_tweet=max(train_features+val_features+test_features, key=len)\n","max_length=len(longest_tweet)\n","padding_index=0 #position 0 is where we had put the padding token in our vocabulary and embedding matrix\n","\n","# padding the feature vectors\n","train_features=[add_padding(vector, max_length, padding_index) for vector in train_features]\n","val_features=[add_padding(vector, max_length, padding_index) for vector in val_features]\n","test_features=[add_padding(vector, max_length, padding_index) for vector in test_features]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7hVz376FkS0k"},"source":["Next, we convert all splits of the data into a PyTorch DataSet as well as a DataLoader, to perform `mini batch gradient descent` when training the model."]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686744511500,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"qI_ukFHukS0l"},"outputs":[],"source":["class TweetEvalTrain(torch.utils.data.Dataset):\n","    # defining the sources of the data\n","    def __init__(self, features, labels):\n","        self.X=torch.LongTensor(features)\n","        self.y=torch.from_numpy(np.array(labels))\n","\n","    def __getitem__(self, index):\n","        X=self.X[index]\n","        y=self.y[index].unsqueeze(0)\n","        return X, y\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","data_train=TweetEvalTrain(train_features, train_labels)\n","data_val=TweetEvalTrain(val_features, val_labels)\n","data_test=TweetEvalTrain(test_features, test_labels)\n","\n","train_loader=torch.utils.data.DataLoader(data_train, batch_size=64)\n","val_loader=torch.utils.data.DataLoader(data_val, batch_size=64)\n","test_loader=torch.utils.data.DataLoader(data_test, batch_size=64)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dJNgsgTfkS0l"},"source":["The data is now ready to be fed into the RNN and LSTM."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5GJ7GZcpkS0m"},"source":["### Many-to-one RNN with pre-trained word embeddings as the inputs"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"R9Jefv2OkS0m"},"source":["First, we define both a custom RNN/LSTM class, a training_loop function and an evaluation function. "]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686744511500,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"jHxzdMI4kS0n"},"outputs":[],"source":["class RNN_LSTM(torch.nn.Module):\n","    def __init__(self, rnn_size, n_classes, embedding_matrix, num_layers=1, type_=\"RNN\"):\n","        super().__init__()\n","        #applying the embeddings to the inputs. Tokens corresponding to the padding_idx will not be included in the training of the model\n","        self.embedding=torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n","        emb_dim=embedding_matrix.shape[1] # Each word is represented through a 200 dimensional vector\n","        self.num_layers=num_layers\n","        self.type_=type_\n","        if self.type_ == \"RNN\":\n","            self.rnn=torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=self.num_layers, batch_first=True)\n","        elif self.type_ == \"LSTM\":\n","            self.rnn=torch.nn.LSTM(input_size=emb_dim, hidden_size=rnn_size, bidirectional=False, num_layers=self.num_layers, batch_first=True)\n","        else:\n","            raise LookupError(\"Only RNN and LSTM are supported.\")\n","        #applies a linear transformation to the RNN/LSTM\n","        self.linear_outputs=torch.nn.Linear(rnn_size, n_classes)\n","\n","    def forward(self, inputs):\n","        # encode the input vectors\n","        encoded_inputs=self.embedding(inputs)\n","        # In this many-to-one model, we only need the final hidden states,\n","        # where all prio information is somehow presen\n","        if self.type_ == \"RNN\":\n","            all_states, final_state=self.rnn(encoded_inputs)\n","            outputs=self.linear_outputs(final_state.squeeze())\n","        else: # if LSTM\n","            all_states, (final_state, c_n)=self.rnn(encoded_inputs)\n","            outputs=self.linear_outputs(final_state.squeeze())\n","        return outputs\n","\n","# training loop\n","def training_loop(model, num_epochs):\n","    loss_function=torch.nn.CrossEntropyLoss() # Note: automatically applies a softmax transformation\n","    optimizer=torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(num_epochs):\n","        losses=[]\n","        for batch_index, (inputs, targets) in enumerate(train_loader):\n","            optimizer.zero_grad() # zero the gradients from the previous opt step\n","            outputs=model.forward(inputs).squeeze() # compute outputs\n","            targets=targets.squeeze().long()\n","            loss=loss_function(outputs, targets) #loss function\n","            loss.backward() # backpropagation - get deriv\n","            optimizer.step() # optimize based on derivative\n","            losses.append(loss.item()) # append batch loss\n","        print(f'Epoch {epoch+1}: loss {np.mean(losses)}')\n","    return model\n","\n","def evaluate(model, val_loader):\n","    predictions=[]\n","    labels=[]\n","    with torch.no_grad(): # don't backpropagate or update weights anymore\n","        for batch_index, (inputs, targets) in enumerate(val_loader):\n","            # apply softmax\n","            outputs=torch.softmax(model(inputs), 1 )\n","            # indices highest softmax values\n","            vals, indices=torch.max(outputs, 1)\n","            # accumulating the predictions\n","            predictions += indices.tolist()\n","            # accumulating true labels\n","            labels += targets.tolist()\n","\n","    acc=accuracy_score(predictions, labels)\n","    f1=f1_score(predictions, labels)\n","    print(f'Model accuracy: {acc:.2f}')\n","    print(f'Model F1: {f1:.2f}')\n","    return acc, f1, predictions"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FcWRBKuJkS0n"},"source":["Train RNN with a single hidden layer and 100 nodes in the layer."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36610,"status":"ok","timestamp":1686744548107,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"PrdH5uULkS0o","outputId":"0c2a7c42-a739-4efd-d972-8e790d23d50b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1: loss 0.6058707241470932\n","Epoch 2: loss 0.550754392189337\n","Epoch 3: loss 0.5269700645977724\n","Epoch 4: loss 0.5149752362400082\n","Epoch 5: loss 0.5122840057873557\n","Model accuracy: 0.68\n","Model F1: 0.66\n"]}],"source":["# initializing and training the model:\n","myRNN=RNN_LSTM(rnn_size=100, n_classes=2, num_layers=1, embedding_matrix=embedding_matrix)\n","\n","myRNN=training_loop(myRNN, num_epochs=5)\n","acc, f1, preds=evaluate(myRNN, val_loader)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RJxU-J4KkS0p"},"source":["\n","### LSTM"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cjLnPY86kS0p"},"source":["A single hidden layer LSTM with 100 nodes in the layer."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80990,"status":"ok","timestamp":1686745006540,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"J5eELmlEkS0p","outputId":"7c0318d4-4b9e-46cd-8dfc-9ccf30d19e3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1: loss 0.5880376453518023\n","Epoch 2: loss 0.5178072105908225\n","Epoch 3: loss 0.4880447679377617\n","Model accuracy: 0.72\n","Model F1: 0.64\n"]}],"source":["myLSTM=RNN_LSTM(rnn_size=100, n_classes=2, num_layers=1, embedding_matrix=embedding_matrix, type_=\"LSTM\")\n","\n","myLSTM=training_loop(myLSTM, num_epochs=3)\n","acc, f1, preds=evaluate(myLSTM, val_loader)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CFnDyB3qkS0r"},"source":["### A fine-tuned BERT model"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":806,"status":"ok","timestamp":1686745007335,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"Z50orgpZQGoQ"},"outputs":[],"source":["metric_f1=load_metric(\"f1\")\n","metric_acc=load_metric(\"accuracy\")\n","\n","# performance function\n","def compute_metrics(eval_pred):\n","      outputs, labels=eval_pred\n","      predictions=np.argmax(outputs, axis=-1)\n","      f1=metric_f1.compute(predictions=predictions, references=labels)\n","      acc=metric_acc.compute(predictions=predictions, references=labels)\n","      return f1 | acc\n","\n","def BERT_hate_classifier(model_name):\n","  # allow model to access the GPU\n","  device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","  # Set up the tokenizer we want to use\n","  tokenizer=AutoTokenizer.from_pretrained(model_name)\n","  # Moving tokenizer to work on GPU\n","  tokenizer.to_device=device\n","  # Apply the tokenizer to each row in the dataset\n","  tokenized_train_dataset=train_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n","  tokenized_val_dataset=val_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n","  tokenized_test_dataset=test_data.map(lambda tweet: tokenizer(tweet[\"text\"]), batched=True).remove_columns(\"text\")\n","  # Specify task for pretrained model\n","  hate_classifier=AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","  # Moving model to GPU\n","  hate_classifier.to(device)\n","  # Specify training parameters\n","  training_args=TrainingArguments(output_dir=\"bert_hatespeech\",\n","                                    evaluation_strategy=\"steps\",\n","                                    num_train_epochs=5,\n","                                    per_device_train_batch_size=16,\n","                                    logging_steps=500,\n","                                    eval_steps=500)\n","\n","  trainer=Trainer(model=hate_classifier,\n","                    args=training_args,\n","                    compute_metrics=compute_metrics,\n","                    train_dataset=tokenized_train_dataset,\n","                    eval_dataset=tokenized_val_dataset,\n","                    tokenizer=tokenizer)\n","  # fine-tune model\n","  trainer.train()\n","\n","  return trainer, tokenized_test_dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4ORn4xWWTKsW"},"source":["We train a BERT model, that is not specialized in twitter hate speech, but a more general purpose BERT trained on Wikipedia and the BookCorpus (Devlin et al. 2017)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvrgZBYdfuUn"},"outputs":[],"source":["trainer, tokenized_test_dataset=  BERT_hate_classifier(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1686743696419,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"Q6hHC1beSXhQ","outputId":"f247f22d-1384-43bf-bc3b-b15bc7d3278e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of fine-tuned BERT model: 0.77\n","F1 of fine-tuned BERT model: 0.74\n"]}],"source":["eval_on_val_data=trainer.evaluate()\n","\n","print(f\"Accuracy of fine-tuned BERT model: {eval_on_val_data['eval_accuracy']:.2f}\")\n","print(f\"F1 of fine-tuned BERT model: {eval_on_val_data['eval_f1']:.2f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YT0qBmJQwBd0"},"source":["Next, to see how much of a difference the pre-trained BERT model makes, we also train a BERT that is developed by the authors behind the TweetEval Hate Speech dataset, and is pre-trained to detect hate speech on twitter (Basile et al. 2019)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjJp97FSv5rS"},"outputs":[],"source":["trainer_tweeteval, tokenized_test_dataset_tweeteval= BERT_hate_classifier(\"cardiffnlp/twitter-roberta-base-hate-latest\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1686743706931,"user":{"displayName":"Esben Lemminger","userId":"11363395418339722852"},"user_tz":-120},"id":"GUd7hTQtv73u","outputId":"dd94650e-ee1e-4fd2-cd99-8f4a2ddc911c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of fine-tuned BERT model: 0.81\n","F1 of fine-tuned BERT model: 0.79\n"]}],"source":["eval_on_val_data=trainer_tweeteval.evaluate()\n","\n","print(f\"Accuracy of fine-tuned BERT model: {eval_on_val_data['eval_accuracy']:.2f}\")\n","print(f\"F1 of fine-tuned BERT model: {eval_on_val_data['eval_f1']:.2f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cG4sF1CLkS0w"},"source":["## For each of the models you ran in question 1. and 2., briefly discuss (two–four sentences) in what ways the model is a good choice for the current task and data set, plus any downsides the model might have for this application.\n","\n","**Logistic Regression**: Since Logistic Regression uses a TF-IDF vectorization to convert words and sentences into numbers, it disregards the order with which the words appear in the sentences. That word ordering does not matter to the meaning of a sentence is a somewhat crude assumption, though with regard to this particular task of detecting hate speech, simply identifying the presence or prevalence of a \"hateful word\" could suffice. This model is also less computationally intensive to train than the rest. \n","\n","**RNN and LSTM**: The fact that both RNN and LSTM have a sequential architecture makes them rather fitting for modeling the semantic meaning of language, compared to the Logistic Regression. The LSTM tries to mitigate the risk of exploding/vanishing gradients that the RNN might suffer from when the input sequence is long (Raschka & Mirjalili 2019:581). Thus, LSTMs are better when it comes to longer sentences. However, tweets are characterized by being short sentences with a limit of 280 characters, and our dataset might not have the proper sentence lengths for the LSTM architecture to fully flourish, compared to the RNN and Logistic Regression. Also, using pretrained embeddings for the text vectorization in RNN and LSTM, the performances of our models are influenced by the corpus used to construct the n-dimensional embedding vector space. We will talk more about this in Section 4.5.\n","\n","**BERT**: BERT also allow for sequential input, which is fitting for language classification. We used two pre-trained BERT-models; One that is trained on twitter data with the objective to detect hate speech, and one that is trained for more general language processing. Both BERT models had better performances on `accuracy` and `F1` than the other models. The model specialized on twitter hate speech performed slightly better than the general purpose BERT. Such pre-trained, specialized model is very convenient, but it also gives the model a natural head-start compared to all the other models. Utilizing the relatively better performance of the BERT models thus depends on the existence of a pre-trained model and also what that model is trained for. Alternatively, we would have had to pre-train the BERT-model from scratch, which would have been a much more extensive task."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NzkqwuB1kS0x"},"source":["## Present the performance of your models on the test set in terms of both F1 and accuracy. Which is the best-performing model in terms of F1? In your opinion, should we prefer F1 over accuracy as an indicator of model performance here, and why?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kQ_9Tj87E6kn"},"source":["We calculate the `accuracy` and `F1` score for each model on the `test data` and print the results."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Logistic Regression</th>\n","      <td>0.49</td>\n","      <td>0.61</td>\n","    </tr>\n","    <tr>\n","      <th>RNN</th>\n","      <td>0.48</td>\n","      <td>0.58</td>\n","    </tr>\n","    <tr>\n","      <th>LSTM</th>\n","      <td>0.53</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>BERT TweetEval</th>\n","      <td>0.57</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>BERT</th>\n","      <td>0.53</td>\n","      <td>0.64</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     Accuracy    F1\n","Logistic Regression      0.49  0.61\n","RNN                      0.48  0.58\n","LSTM                     0.53  0.60\n","BERT TweetEval           0.57  0.65\n","BERT                     0.53  0.64"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["test_pred_lr=lr_grid.predict(test_features_lr) # LLR\n","acc_rnn, f1_rnn, preds=evaluate(myRNN, test_loader) # RNN\n","acc_lstm, f1_lstm, preds=evaluate(myLSTM, test_loader) # LSTM\n","\n","output, true_labels, eval=trainer.predict(tokenized_test_dataset) # BERT\n","acc_bert, f1_bert=eval[\"test_accuracy\"], eval[\"test_f1\"]\n","\n","output, true_labels, eval_tweeteval=trainer_tweeteval.predict(tokenized_test_dataset_tweeteval) # TweetEval BERT\n","acc_bert_tweeteval, f1_bert_tweeteval=eval_tweeteval[\"test_accuracy\"], eval_tweeteval[\"test_f1\"]\n","\n","pd.DataFrame([[accuracy_score(test_pred_lr, test_labels),\n","               f1_score(test_pred_lr, test_labels)],\n","              [acc_rnn, f1_rnn],\n","              [acc_lstm, f1_lstm],\n","              [acc_bert_tweeteval, f1_bert_tweeteval],\n","              [acc_bert, f1_bert]],\n","             index=[\"Logistic Regression\", \"RNN\", \"LSTM\", \"BERT TweetEval\", \"BERT\"],\n","             columns=[\"Accuracy\", \"F1\"]).round(2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["First of all, we notice that all models perform significantly worse on the test data, compared to their performance on the validation data. This could obviously be a modeling issue, but it could also encourage a closer look at the test datasets, to see if it comes down to an issue of noise or messy data. The results, however, are close to the one's measured by the authors of the `TweetEval Hate Speech` dataset (Basile et al. 2019). Also, whether various forms of preprocessing could have increased model performance would have to be explored if one were to further fine-tune the models for production or publication.\n","\n","Deciding how to weigh the importance of either the `accuracy` or `F1` depends on the task and what one wants to achieve - or not to achieve - with one's classification model. Accuracy has straightforward interpretability and is a good metric when the outcome classes are reasonably balanced, as is the case in our dataset. The `F1-score`, on the other hand, is less intuitively interpretable, but is a better pick for imbalanced classes. `F1` takes the `False Positives` and `False Negatives` into account, assessing both error types relative to the number of `True Positives`, rather than the `True Negatives` (Hovy 2022b: 25f). This also means that *F1* changes, depending on which class we \"focus on\" - that is, which class we assign to be a `True Positive`. In our case, we focus on hate rather than non-hate. So, is that diserable? Since our ultimate goal is hate speech detection, *F1* could be a more adequate measure, in spite of the balanced classes. Adhering to best practice, we present both metrics to provide a comprehensive evaluation of our model´s performance, and, either way, the BERT pre-trained on twitter hate speech has the better performance on both metrics, with an accuracy of 0.57 and F1 of 0.65. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"afI5KdCRkS00"},"source":["## Based on the paper by Basile et al. (2019), What further info might you have liked to have about the data selection process and/or the annotation process for this data set? Why?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_REAg-dn8bSU"},"source":["In the **data selection process**, the term `identified hater` is not further defined (Basile et al. 2019: 55). Thus, the exact sampling strategy is unclear which makes replication difficult.\n","\n","In the **annotation process**, the authors try to counter the potential inconsistency of the individual annotator by using majority voting on each tweet among three annotators (crowds). The tweet is also annotated by two experts, whereafter the final label is a majority voting between crowd, expert 1, and expert 2. The exact qualifications of the experts remains unclear in the article, aside from language capability and general field knowledge. The article provides a measure of the average confidence (AC) between the annotations, but it is unclear if this measure only relates to the crowd. It would have been informative to explicitly have both the AC between the crowd and between the crowd and the experts. Though the aggregate AC measure is informative, it could have been interesting to include a measure of agreement/reliability for each tweet in the dataset, as to get an indication of which tweets have been unanimously annotated and which one's have been more ambiguous annotated.\n","\n","Finally, we would have liked if the authors were more explicit about the constitution of their crowdsourcing provider and annotators, especially since crowdsourcing of annotative labor has a history of irresponsible working conditions. This also raise a discursive consideration, as it could also be interesting to know more about the social characteristics of the annotators and experts. Are they all from the same country or do they all identify as the same gender? There might be discursive reminiscents of these social dimensions latently embedded in the annotation of what is hate speech and what is not in the data. \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Part 4: Word embeddings"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The Word2vec algorithm is a widely used embedding implementation introduced by Le and Milov (2014) (Grimmer et al. 2022: 86; Hovy 2022b: 19). The assumption behind word2vec is that words that appear in similar contexts have similar meaning. Based on this assumption, the algorithm creates an n-dimensional vector space, where the meaning of each word is reflected by its relative position to other words. In Section 1, we saw how OHR utilized this to identify the emotiveness of non-labelled words based on their relative proximity to either emotive or neutral words in an embedding space. In this section, we will train a word2vec embedding model on the United Nations General Debate Corpus, and explore the strengths and weaknesses of word embedding for semantic analysis. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train your own word2vec vectors on the dataset. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We load, tokenize, and lowercase the data."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def load_and_process_data(filename):\n","    # to hold all sentences in the corpus\n","    corpus=[]\n","    \n","    # Open the file \n","    with open(filename, \"r\", encoding=\"utf-8\") as f:\n","        for line in f: # iterate over lines\n","            # is line empty\n","            if line.strip() != '':\n","                # Tokenize and lowercase \n","                encoded_text=[word.lower() for word in word_tokenize(line)]\n","                # Add tokens to the corpus\n","                corpus.append(encoded_text)\n","    \n","    return corpus\n","\n","# load and process the datab\n","path=\"Data\"\n","speeches=load_and_process_data(path+\"/allspeeches_77_2022.txt\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, we train our word2vec model."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def vec_function(seed):\n","    speech2vec=gensim.models.Word2Vec(\n","        speeches,         # the corpus object we've loaded\n","        vector_size=200,  # the dimensionality of the target vectors\n","        window=5,         # window ngram size\n","        min_count=4,      # ignoring low-frequency words\n","        epochs=3,         # how many training passes to have\n","        sg=1,           # 1 for skip-gram model\n","        seed=seed)        # seed for replication\n","    return(speech2vec)\n","\n","vec_seed13=vec_function(seed=13)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are two architectures which can be implemented in the Word2Vec; Skip-Gram(SG) or Continous Bag of Words (CBOW). In general SG outperforms CBOW, so we will use that approach for this exercise (Ogundepo 2021). Further, we choose the default of negative sampling for updating the weights in the neural network during training, to minimize the computational intensity. \n","\n","Aside from the parameters provided in the assignment description, we set a seed for replication, select the skip-gram model and negative samping, and choose to ignore low frequency words. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Print out the ten words that are most similar to: “climate”, “pandemic”, “terrorism” and “future” (or choose your own four words of interest). Briefly discuss anything you find noteworthy about the associations that the model has picked up."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We use the `most_similar` method, which measures the distance between words in the vector space using cosine-similarity,  "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>gender</th>\n","      <th>elizabeth</th>\n","      <th>russia</th>\n","      <th>ukraine</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>protection</td>\n","      <td>king</td>\n","      <td>ukraine</td>\n","      <td>russia</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>empowerment</td>\n","      <td>majesty</td>\n","      <td>illegal</td>\n","      <td>military</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>women</td>\n","      <td>excellency</td>\n","      <td>military</td>\n","      <td>aggression</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>services</td>\n","      <td>abdulla</td>\n","      <td>aggression</td>\n","      <td>illegal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>girls</td>\n","      <td>shahid</td>\n","      <td>russian</td>\n","      <td>russian</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>equality</td>\n","      <td>abdullah</td>\n","      <td>invasion</td>\n","      <td>forces</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>employment</td>\n","      <td>charles</td>\n","      <td>federation</td>\n","      <td>armed</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>promotion</td>\n","      <td>csaba</td>\n","      <td>forces</td>\n","      <td>crimes</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>public</td>\n","      <td>predecessor</td>\n","      <td>iran</td>\n","      <td>war</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>participation</td>\n","      <td>maldives</td>\n","      <td>war</td>\n","      <td>federation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          gender    elizabeth      russia     ukraine\n","0     protection         king     ukraine      russia\n","1    empowerment      majesty     illegal    military\n","2          women   excellency    military  aggression\n","3       services      abdulla  aggression     illegal\n","4          girls       shahid     russian     russian\n","5       equality     abdullah    invasion      forces\n","6     employment      charles  federation       armed\n","7      promotion        csaba      forces      crimes\n","8         public  predecessor        iran         war\n","9  participation     maldives         war  federation"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["word_of_interest=[\"gender\", \"elizabeth\", \"russia\", \"ukraine\"] \n","\n","#  func to find top ten most similar words\n","def find_top_words(word_of_interest, model):\n","    df=pd.DataFrame() \n","    for word in word_of_interest:\n","        # Using the most_similar function to find the 10 words\n","        similar_words=list(map(lambda x: x[0], model.wv.most_similar(word, 10)))\n","        df[word]=similar_words\n","    return(df) # Returning a dataframe\n","\n","top_words_seed13=find_top_words(word_of_interest=word_of_interest, model= vec_seed13)\n","top_words_seed13"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Starting with `gender`, we see that *empowerment*, *protection*, and *women* are the most similar words to gender. This would indicate that these words often appear in the similar contexts. It is interesting that *men* is not one of the most similar word to gender, while *women* is. This could indicate that debates on gender in the UN is mainly concerning empowerment and protection of women rather than men. A rough Beauvoirian interpretation of this could be that men are considered the *default* gender, and women is the *other*.\n","\n","Looking at `elisabeth`, it makes sense that *king*, *charles*, and *iii* are closely positioned in the vector space since they probably refer to the Queens death in 2022 and her son and successor King Charles III. Oddly, the words *abdulla(h)* and *shahid* also appear as similar, which probably refers to Abdullah Shahid, a key politician within the UN, who is often addressed as \"his exellence\" in the text corpus. \n","\n","Both `russia` and `ukraine` respectively have each other as the most similar words. It makes sense then, that they also share many of the same words as their most similar ones, e.g. military, aggresion, and war. Since they are both each others most similar word, they are positioned rather close to each other in the vector space, therefore also sharing their proximity to all the other words. This makes it difficult to discern which words are targeting which of the countries, e.g. is the similarity between *illigal* and *ukrain* mainly because ukrain is close to russia in the vector space? And which one of the countries are commiting the *aggresion*?  "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Suppose that we would like to use these word embeddings as input in a supervised model, detecting whether the speech comes from a country in the global North or the global South. Briefly discuss the upside(s) and downside(s) of training your word embeddings locally, on the speeches themselves, versus using pre-trained embeddings."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Deciding whether to train our own word embeddings locally or use pre-trained embeddings, it is important to consider how similar or different one's text corpus of interest is to the copera of the pre-trained embeddings (Rodriguez & Spirling 2022: 6). Using pre-trained embeddings comes with the risk of inaccuracy due to lacking domain specificity compared to one's corpus of interest - say we load embeddings trained on Wikipedia to use for our UN speech analysis (Rodriguez & Spirling 2022: 6). One could also encounter that words are not within the pre-trained embedding's vocabulary, thus losing the potential information in out-of-vocabulary words (Grimmer et al. 2022: 84f). On the other, data accessibility and computing power can be a problem for locally trained embeddings. The pre-trained embeddings are often trained on hundreds of billions of tokens, while we only have 77 speeches in the UN. If the copora of the pre-trained embeddings is related to one's local corpus - for instance if both are texts on political speecehs - one can save some computation by opting for pre-trained embeddings. In general Rodriguez & Spirling find that the pre-trained embeddings often do better than the locally trained models (Rodriguez & Spirling 2022: 15-16). \n","\n","A limitation to the embedding framework is that the vector representations are fixed when training is completed, meaning that the exact context of the words are average out (Grimmer et al. 2022: 88). Thus, discerning between global North or global South could be difficult using word2vec's vector representations, as the potential contextual nuaces and differences of a particular word between North and South is aggregated across all speeches and fixed into its average contextual meaning. In newer models such as BERT, they have implemented contextual embeddings, which are better at capturing semantic nuances in the language. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## (optional) Train the model again. Are the word embeddings stable?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Not answered due to character limitation."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## (optional) Conduct an informal validation of the embeddings from your first run, by checking their ability to find the “odd one out” in three different series of four–five terms related to international relations and current events (e.g. “covid”, “pandemic”, “disease”, “vaccine”, “environment”). Briefly discuss how you might validate the embeddings more systematically, if you had more time and resources. "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["For 'war' the odd one out is: guns\n","For 'climate' the odd one out is: floods\n","For 'nations' the odd one out is: russia\n","For 'covid 19' the odd one out is: environment\n"]}],"source":["list_of_lists_of_words={\n","    \"war\": [\"bombs\", \"guns\", \"ammo\", \"tanks\" \"cake\"],\n","    \"climate\": [\"floods\", \"drought\", \"heatwave\", \"hallo\"],\n","    \"nations\": [\"china\", \"america\", \"russia\", \"hammer\"],\n","    \"covid 19\": [\"covid\", \"pandemic\", \"disease\", \"vaccine\", \"environment\"]\n","}\n","for key, value in list_of_lists_of_words.items():\n","    print(f\"For '{key}' the odd one out is: {vec_seed13.wv.doesnt_match(value)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The model does not do a particularly good job of identifying the \"odd one out\". It only gets ´environment´ right in the covid 19 list. If one had the time and ressources, a more systematic validation could be conducted using the approach presented by Rodriguez & Spirling (2021). For a given target word, they construct two seperate lists of the 10 most similar words according to either human annotators or their embedding model (Rodriguez & Spirling 2021: 11). They then ask a seperate group of humans to choose whether the embedding model or the human annotators have provided the most fitting list of similar words. By applying this approach to a subset of relevant target words, it could provide a somewhat systematic, yet rather costly validation of one's embedding model. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gvm_xNm3JGfm"},"source":["## Literature\n","\n","**Basile, V., Bosco, C., Fersini, E., Nozza, D., Patti, V., Rangel Pardo, F. M., Rosso, P., & Sanguinetti, M.** (2019). SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter. In *Proceedings of the 13th International Workshop on Semantic Evaluation* (pp. 54-63). Minneapolis, Minnesota, USA: Association for Computational Linguistics. [Link](https://www.aclweb.org/anthology/S19-2007) (DOI: [10.18653/v1/S19-2007](https://doi.org/10.18653/v1/S19-2007))\n","\n","**Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova-** 2018. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" CoRR, abs/1810.04805. Retrieved from http://arxiv.org/abs/1810.04805.\n","\n","**Raschka, Sebastian, V. Mirjalili** (2019). Python Machine Learning;Machine Learning and Deep Learning with Python Scikit-Learn and Tensorflow 2 3rd Edition. https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=2329991. Accessed June 14 2023.\n","\n","**Ogundepo, Odunayo**  (2021): \"Understanding Word2Vec\", Medium, https://medium.com/analytics-vidhya/understanding-word2vec-39fabe660705"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
